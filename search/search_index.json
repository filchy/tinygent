{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#what-is-tinygent","title":"What is Tinygent?","text":"<p>Tinygent is a lightweight, powerful agentic framework for building generative AI applications. Unlike heavyweight frameworks that come with complexity overhead, Tinygent focuses on simplicity, flexibility, and developer experience.</p> <p>Built with modern Python best practices, Tinygent provides:</p> <ul> <li>Simple API - Build agents in just a few lines of code</li> <li>Multi-Provider Support - OpenAI, Anthropic, Mistral, Gemini, and more</li> <li>Flexible Tools - Simple, reasoning, and JIT tool decorators</li> <li>Smart Memory - Buffer, summary, window, and combined memory types</li> <li>Middleware - Extensible agent behavior with middleware pattern</li> <li>Async-First - Built for high-performance async workflows</li> <li>Modular Design - Use only what you need</li> </ul>"},{"location":"#why-tinygent","title":"Why Tinygent?","text":""},{"location":"#start-simple-scale-your-way","title":"Start Simple, Scale Your Way","text":"<p>Tinygent follows the principle of progressive disclosure - start simple, grow complex only when needed.</p> <pre><code>from tinygent.tools import tool\nfrom tinygent.core.factory import build_agent\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather in a given location.\"\"\"\n    return f'The weather in {location} is sunny with a high of 75\u00b0F.'\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n)\n\nresult = agent.run('What is the weather like in Prague?')\nprint(result)\n</code></pre> <p>That's it! You've built a ReAct agent with tool-calling capabilities.</p>"},{"location":"#provider-agnostic","title":"Provider Agnostic","text":"<p>Switch between LLM providers with a single string:</p> <pre><code># OpenAI\nagent = build_agent('react', llm='openai:gpt-4o-mini', tools=[...])\n\n# Anthropic Claude\nagent = build_agent('react', llm='anthropic:claude-3-5-sonnet', tools=[...])\n\n# Mistral\nagent = build_agent('react', llm='mistralai:mistral-large-latest', tools=[...])\n\n# Google Gemini\nagent = build_agent('react', llm='gemini:gemini-2.0-flash-exp', tools=[...])\n</code></pre>"},{"location":"#batteries-included-but-replaceable","title":"Batteries Included, But Replaceable","text":"<p>Tinygent comes with powerful built-in components, but everything is replaceable:</p> <ul> <li>4 Agent Types: ReAct, MultiStep, Squad, MAP (Modular Agentic Planner)</li> <li>Multiple Memory Types: Buffer, Summary, Window, Combined</li> <li>Tool Decorators: <code>@tool</code>, <code>@reasoning_tool</code>, <code>@jit_tool</code></li> <li>Middleware System: Extensible hooks for customization</li> <li>Optional Packages: Brave search, Neo4j graphs, Chat UI, and more</li> </ul>"},{"location":"#core-principles","title":"Core Principles","text":""},{"location":"#1-configbuilder-pattern","title":"1. Config/Builder Pattern","text":"<p>All components use a consistent configuration pattern:</p> <pre><code>from tinygent.agents.react_agent import ReactAgentConfig\n\nconfig = ReactAgentConfig(\n    llm=\"openai:gpt-4o-mini\",\n    tools=[get_weather],\n    max_iterations=5,\n    temperature=0.7\n)\n\nagent = config.build()\n</code></pre>"},{"location":"#2-registry-pattern","title":"2. Registry Pattern","text":"<p>Components auto-register for global discovery:</p> <pre><code>from tinygent.tools import register_tool\n\n@register_tool\ndef search(query: str) -&gt; str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for: {query}\"\n\n# Later, discover and use from anywhere\nfrom tinygent.core.runtime.tool_catalog import GlobalToolCatalog\n\nregistry = GlobalToolCatalog().get_active_catalog()\nsearch_tool = registry.get_tool('search')\n</code></pre>"},{"location":"#3-async-first","title":"3. Async-First","text":"<p>All agent operations support async streaming:</p> <pre><code>async def main():\n    agent = build_agent('react', llm='openai:gpt-4o-mini', tools=[...])\n\n    # Stream responses\n    async for chunk in agent.run_stream('What is the weather?'):\n        print(chunk, end='', flush=True)\n\n# Synchronous wrapper available too\nresult = agent.run('What is the weather?')\n</code></pre>"},{"location":"#4-type-safety","title":"4. Type Safety","text":"<p>Built with Pydantic for runtime validation and excellent IDE support:</p> <pre><code>from pydantic import Field\nfrom tinygent.core.types import TinyModel\n\nclass WeatherInput(TinyModel):\n    location: str = Field(..., description='The location to get weather for')\n    units: str = Field('celsius', description='Temperature units')\n\n@tool\ndef get_weather(data: WeatherInput) -&gt; str:\n    return f\"Weather in {data.location}: 22\u00b0{data.units[0].upper()}\"\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone git@github.com:filchy/tinygent.git\ncd tinygent\n\n# Create virtual environment\nuv venv --seed .venv\nsource .venv/bin/activate\n\n# Install core library\nuv sync\n\n# Install with OpenAI support\nuv sync --extra openai\n</code></pre>"},{"location":"#your-first-agent","title":"Your First Agent","text":"<p>Create a file <code>my_agent.py</code>:</p> <pre><code>from tinygent.tools import tool\nfrom tinygent.core.factory import build_agent\n\n@tool\ndef calculator(expression: str) -&gt; str:\n    \"\"\"Evaluate a mathematical expression.\"\"\"\n    try:\n        result = eval(expression)\n        return f\"The result is: {result}\"\n    except Exception as e:\n        return f\"Error: {e}\"\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[calculator],\n)\n\nresult = agent.run('What is 123 * 456?')\nprint(result)\n</code></pre> <p>Run it:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key\"\nuv run my_agent.py\n</code></pre>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p> Getting Started</p> <p>Install Tinygent and build your first agent in minutes</p> <p> Installation Guide</p> </li> <li> <p> Core Concepts</p> <p>Learn about agents, tools, LLMs, memory, and middleware</p> <p> Learn Concepts</p> </li> <li> <p> Examples</p> <p>Explore practical examples and use cases</p> <p> View Examples</p> </li> <li> <p> API Reference</p> <p>Detailed API documentation for all components</p> <p> API Docs</p> </li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: github.com/filchy/tinygent</li> <li>Issues: Report bugs or request features</li> <li>Examples: Check the <code>examples/</code> directory for more code samples</li> </ul>"},{"location":"#license","title":"License","text":"<p>Tinygent is open source software. Check the repository for license details.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API reference for Tinygent.</p>"},{"location":"api-reference/#core-factory-functions","title":"Core Factory Functions","text":""},{"location":"api-reference/#build_agent","title":"<code>build_agent()</code>","text":"<p>Build an agent from a string identifier.</p> <pre><code>from tinygent.core.factory import build_agent\n\nagent = build_agent(\n    agent_type: str,\n    llm: str | BaseLLM,\n    tools: list[AbstractTool] = [],\n    memory: BaseMemory | None = None,\n    middleware: list[TinyBaseMiddleware] = [],\n    max_iterations: int = 5,\n    **kwargs\n) -&gt; BaseAgent\n</code></pre> <p>Parameters:</p> <ul> <li><code>agent_type</code> (str): Agent type identifier (<code>'react'</code>, <code>'multi_step'</code>, <code>'squad'</code>, <code>'map'</code>)</li> <li><code>llm</code> (str | BaseLLM): LLM identifier (e.g., <code>'openai:gpt-4o-mini'</code>) or LLM instance</li> <li><code>tools</code> (list): List of tool functions</li> <li><code>memory</code> (BaseMemory | None): Memory instance</li> <li><code>middleware</code> (list): List of middleware instances</li> <li><code>max_iterations</code> (int): Maximum reasoning iterations</li> </ul> <p>Returns: Agent instance</p> <p>Example:</p> <pre><code>agent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n    max_iterations=10\n)\n</code></pre>"},{"location":"api-reference/#build_llm","title":"<code>build_llm()</code>","text":"<p>Build an LLM from a provider string.</p> <pre><code>from tinygent.core.factory import build_llm\n\nllm = build_llm(\n    llm_string: str,\n    temperature: float = 0.7,\n    max_tokens: int | None = None,\n    **kwargs\n) -&gt; BaseLLM\n</code></pre> <p>Parameters:</p> <ul> <li><code>llm_string</code> (str): Provider and model (<code>'provider:model'</code>)</li> <li><code>temperature</code> (float): Sampling temperature (0.0-2.0)</li> <li><code>max_tokens</code> (int | None): Maximum tokens to generate</li> </ul> <p>Returns: LLM instance</p> <p>Example:</p> <pre><code>llm = build_llm('openai:gpt-4o-mini', temperature=0.3, max_tokens=1000)\n</code></pre>"},{"location":"api-reference/#build_memory","title":"<code>build_memory()</code>","text":"<p>Build a memory instance from configuration.</p> <pre><code>from tinygent.core.factory import build_memory\n\nmemory = build_memory(\n    memory_type: str,\n    **kwargs\n) -&gt; BaseMemory\n</code></pre> <p>Parameters:</p> <ul> <li><code>memory_type</code> (str): Memory type identifier</li> <li><code>**kwargs</code>: Memory-specific parameters</li> </ul> <p>Returns: Memory instance</p>"},{"location":"api-reference/#build_tool","title":"<code>build_tool()</code>","text":"<p>Build a tool from configuration.</p> <pre><code>from tinygent.core.factory import build_tool\n\ntool = build_tool(\n    tool_name: str,\n    **kwargs\n) -&gt; AbstractTool\n</code></pre>"},{"location":"api-reference/#build_embedder","title":"<code>build_embedder()</code>","text":"<p>Build an embedder for vector embeddings.</p> <pre><code>from tinygent.core.factory import build_embedder\n\nembedder = build_embedder(\n    embedder_string: str,\n    **kwargs\n) -&gt; BaseEmbedder\n</code></pre> <p>Example:</p> <pre><code>embedder = build_embedder('openai:text-embedding-3-small')\nvectors = embedder.embed_documents(['Hello', 'World'])\n</code></pre>"},{"location":"api-reference/#agents","title":"Agents","text":""},{"location":"api-reference/#reactagent","title":"ReActAgent","text":"<pre><code>from tinygent.agents.react_agent import TinyReActAgent\n\nagent = TinyReActAgent(\n    llm: BaseLLM,\n    tools: list[AbstractTool],\n    memory: BaseMemory | None = None,\n    middleware: list[TinyBaseMiddleware] = [],\n    max_iterations: int = 5,\n    prompt_template: ReActPromptTemplate | None = None,\n)\n</code></pre> <p>Methods:</p> <ul> <li><code>run(task: str) -&gt; str</code>: Execute task synchronously</li> <li><code>run_stream(task: str) -&gt; AsyncIterator[str]</code>: Execute with streaming</li> <li><code>reset()</code>: Clear agent state</li> </ul>"},{"location":"api-reference/#multistepagent","title":"MultiStepAgent","text":"<pre><code>from tinygent.agents.multi_step_agent import TinyMultiStepAgent\n\nagent = TinyMultiStepAgent(\n    llm: BaseLLM,\n    tools: list[AbstractTool],\n    memory: BaseMemory | None = None,\n    middleware: list[TinyBaseMiddleware] = [],\n    max_iterations: int = 10,\n    prompt_template: MultiStepPromptTemplate | None = None,\n)\n</code></pre>"},{"location":"api-reference/#squadagent","title":"SquadAgent","text":"<pre><code>from tinygent.agents.squad_agent import TinySquadAgent\n\nsquad = TinySquadAgent(\n    llm: BaseLLM,\n    agents: list[BaseAgent],\n    memory: BaseMemory | None = None,\n    middleware: list[TinyBaseMiddleware] = [],\n    max_iterations: int = 5,\n)\n</code></pre>"},{"location":"api-reference/#mapagent","title":"MAPAgent","text":"<pre><code>from tinygent.agents.map_agent import TinyMAPAgent\n\nagent = TinyMAPAgent(\n    llm: BaseLLM,\n    tools: list[AbstractTool],\n    memory: BaseMemory | None = None,\n    middleware: list[TinyBaseMiddleware] = [],\n    max_iterations: int = 15,\n)\n</code></pre>"},{"location":"api-reference/#tool-decorators","title":"Tool Decorators","text":""},{"location":"api-reference/#tool","title":"<code>@tool</code>","text":"<p>Create a simple tool.</p> <pre><code>from tinygent.tools import tool\n\n@tool\ndef my_function(param: str) -&gt; str:\n    \"\"\"Function description.\"\"\"\n    return result\n</code></pre>"},{"location":"api-reference/#register_tool","title":"<code>@register_tool</code>","text":"<p>Create and globally register a tool.</p> <pre><code>from tinygent.tools import register_tool\n\n@register_tool(use_cache: bool = False, hidden: bool = False)\ndef my_function(param: str) -&gt; str:\n    \"\"\"Function description.\"\"\"\n    return result\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_cache</code> (bool): Enable result caching</li> <li><code>hidden</code> (bool): Hide from default tool listings</li> </ul>"},{"location":"api-reference/#reasoning_tool","title":"<code>@reasoning_tool</code>","text":"<p>Create a tool requiring reasoning.</p> <pre><code>from tinygent.tools import register_reasoning_tool\n\n@register_reasoning_tool(reasoning_prompt: str)\ndef my_function(param: str) -&gt; str:\n    \"\"\"Function description.\"\"\"\n    return result\n</code></pre> <p>Parameters:</p> <ul> <li><code>reasoning_prompt</code> (str): Prompt for agent reasoning</li> </ul>"},{"location":"api-reference/#jit_tool","title":"<code>@jit_tool</code>","text":"<p>Create a just-in-time code generation tool.</p> <pre><code>from tinygent.tools import jit_tool\n\n@jit_tool(jit_instruction: str)\ndef my_function(param: str):\n    \"\"\"Function description.\"\"\"\n    yield result\n</code></pre> <p>Parameters:</p> <ul> <li><code>jit_instruction</code> (str): Code generation instructions</li> </ul>"},{"location":"api-reference/#memory","title":"Memory","text":""},{"location":"api-reference/#bufferchatmemory","title":"BufferChatMemory","text":"<pre><code>from tinygent.memory import BufferChatMemory\n\nmemory = BufferChatMemory()\n</code></pre> <p>Methods:</p> <ul> <li><code>save_context(message: TinyMessage)</code>: Save a message</li> <li><code>load_variables() -&gt; list[TinyMessage]</code>: Load all messages</li> <li><code>clear()</code>: Clear all messages</li> </ul>"},{"location":"api-reference/#summarybuffermemory","title":"SummaryBufferMemory","text":"<pre><code>from tinygent.memory import SummaryBufferMemory\n\nmemory = SummaryBufferMemory(\n    llm: BaseLLM,\n    max_token_limit: int = 1000,\n)\n</code></pre>"},{"location":"api-reference/#windowbuffermemory","title":"WindowBufferMemory","text":"<pre><code>from tinygent.memory import WindowBufferMemory\n\nmemory = WindowBufferMemory(\n    window_size: int = 4,\n)\n</code></pre>"},{"location":"api-reference/#combinedmemory","title":"CombinedMemory","text":"<pre><code>from tinygent.memory import CombinedMemory\n\nmemory = CombinedMemory(\n    memories: dict[str, BaseMemory],\n)\n</code></pre>"},{"location":"api-reference/#middleware","title":"Middleware","text":""},{"location":"api-reference/#base-middleware","title":"Base Middleware","text":"<pre><code>from tinygent.agents.middleware import TinyBaseMiddleware\n\nclass CustomMiddleware(TinyBaseMiddleware):\n    def on_start(self, *, run_id: str, task: str) -&gt; None:\n        pass\n\n    def on_end(self, *, run_id: str) -&gt; None:\n        pass\n\n    def on_error(self, *, run_id: str, e: Exception) -&gt; None:\n        pass\n\n    def before_llm_call(self, *, run_id: str, llm_input) -&gt; None:\n        pass\n\n    def after_llm_call(self, *, run_id: str, llm_input, result) -&gt; None:\n        pass\n\n    def before_tool_call(self, *, run_id: str, tool, args: dict) -&gt; None:\n        pass\n\n    def after_tool_call(self, *, run_id: str, tool, args: dict, result) -&gt; None:\n        pass\n\n    def on_reasoning(self, *, run_id: str, reasoning: str) -&gt; None:\n        pass\n\n    def on_answer(self, *, run_id: str, answer: str) -&gt; None:\n        pass\n\n    def on_answer_chunk(self, *, run_id: str, chunk: str, idx: str) -&gt; None:\n        pass\n</code></pre>"},{"location":"api-reference/#register-middleware","title":"Register Middleware","text":"<pre><code>from tinygent.agents.middleware import TinyBaseMiddleware\n\n@register_middleware('my_middleware')\nclass MyMiddleware(TinyBaseMiddleware):\n    # ...\n</code></pre>"},{"location":"api-reference/#messages","title":"Messages","text":""},{"location":"api-reference/#message-types","title":"Message Types","text":"<pre><code>from tinygent.core.datamodels.messages import (\n    TinyHumanMessage,\n    TinyChatMessage,\n    TinySystemMessage,\n    TinyPlanMessage,\n    TinyToolMessage,\n)\n\n# Create messages\nhuman_msg = TinyHumanMessage(content=\"Hello\")\nai_msg = TinyChatMessage(content=\"Hi there!\")\nsystem_msg = TinySystemMessage(content=\"You are a helpful assistant\")\n</code></pre>"},{"location":"api-reference/#data-models","title":"Data Models","text":""},{"location":"api-reference/#tinymodel","title":"TinyModel","text":"<p>Base class for Pydantic models.</p> <pre><code>from pydantic import Field\nfrom tinygent.core.types import TinyModel\n\nclass MyInput(TinyModel):\n    name: str = Field(..., description='User name')\n    age: int = Field(..., ge=0, description='User age')\n</code></pre>"},{"location":"api-reference/#runtime-registry","title":"Runtime Registry","text":""},{"location":"api-reference/#tool-catalog","title":"Tool Catalog","text":"<pre><code>from tinygent.core.runtime.tool_catalog import GlobalToolCatalog\n\n# Get catalog\ncatalog = GlobalToolCatalog().get_active_catalog()\n\n# List tools\ntools = catalog.list_tools()\n\n# Get specific tool\ntool = catalog.get_tool('tool_name')\n\n# Call tool\nresult = tool(param='value')\n</code></pre>"},{"location":"api-reference/#global-registry","title":"Global Registry","text":"<pre><code>from tinygent.core.runtime.global_registry import (\n    get_registered_agents,\n    get_registered_llms,\n    get_registered_tools,\n    get_registered_memories,\n)\n\n# Get all registered components\nagents = get_registered_agents()\nllms = get_registered_llms()\ntools = get_registered_tools()\nmemories = get_registered_memories()\n</code></pre>"},{"location":"api-reference/#llm-usage","title":"LLM Usage","text":""},{"location":"api-reference/#direct-llm-calls","title":"Direct LLM Calls","text":"<pre><code>from tinygent.core.factory import build_llm\n\nllm = build_llm('openai:gpt-4o-mini')\n\n# Synchronous\nresponse = llm.generate(prompt=\"What is AI?\")\nprint(response.content)\n\n# Asynchronous\nresponse = await llm.agenerate(prompt=\"What is AI?\")\n\n# Streaming\nasync for chunk in llm.stream(prompt=\"Tell me a story\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"api-reference/#logging","title":"Logging","text":""},{"location":"api-reference/#setup-logger","title":"Setup Logger","text":"<pre><code>from tinygent.logging import setup_logger\n\n# Debug level\nlogger = setup_logger('debug')\n\n# Info level\nlogger = setup_logger('info')\n\n# Use logger\nlogger.info(\"Message\")\nlogger.debug(\"Debug message\")\nlogger.error(\"Error message\")\n</code></pre>"},{"location":"api-reference/#utilities","title":"Utilities","text":""},{"location":"api-reference/#color-printer","title":"Color Printer","text":"<pre><code>from tinygent.utils import TinyColorPrinter\n\n# Predefined colors\nprint(TinyColorPrinter.success(\"Success!\"))\nprint(TinyColorPrinter.error(\"Error!\"))\nprint(TinyColorPrinter.warning(\"Warning!\"))\nprint(TinyColorPrinter.info(\"Info\"))\n\n# Custom color\nprint(TinyColorPrinter.custom(\"Label\", \"Message\", color=\"CYAN\"))\n</code></pre>"},{"location":"api-reference/#yaml-loader","title":"YAML Loader","text":"<pre><code>from tinygent.utils import tiny_yaml_load\n\nconfig = tiny_yaml_load('config.yaml')\n</code></pre>"},{"location":"api-reference/#type-hints","title":"Type Hints","text":""},{"location":"api-reference/#common-types","title":"Common Types","text":"<pre><code>from typing import List, Dict, Optional, Any\nfrom tinygent.core.types import TinyModel\nfrom tinygent.core.datamodels.tool import AbstractTool\nfrom tinygent.core.datamodels.messages import TinyMessage\nfrom tinygent.agents.base import BaseAgent\nfrom tinygent.memory import BaseMemory\nfrom tinygent.llms.base import BaseLLM\n</code></pre>"},{"location":"api-reference/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api-reference/#reactagentconfig","title":"ReactAgentConfig","text":"<pre><code>from tinygent.agents.react_agent import ReactAgentConfig\n\nconfig = ReactAgentConfig(\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    max_iterations=5,\n    memory=memory,\n    middleware=[...],\n)\n\nagent = config.build()\n</code></pre>"},{"location":"api-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started: Setup guide</li> <li>Core Concepts: Learn fundamentals</li> <li>Examples: See code examples</li> </ul>"},{"location":"api-reference/#package-structure","title":"Package Structure","text":"<pre><code>tinygent/\n\u251c\u2500\u2500 agents/          # Agent implementations\n\u251c\u2500\u2500 core/            # Core functionality\n\u2502   \u251c\u2500\u2500 datamodels/  # Data models\n\u2502   \u251c\u2500\u2500 factory/     # Factory functions\n\u2502   \u251c\u2500\u2500 prompts/     # Prompt templates\n\u2502   \u251c\u2500\u2500 runtime/     # Runtime registries\n\u2502   \u2514\u2500\u2500 types/       # Type definitions\n\u251c\u2500\u2500 llms/            # LLM integrations\n\u251c\u2500\u2500 memory/          # Memory implementations\n\u251c\u2500\u2500 tools/           # Tool decorators\n\u251c\u2500\u2500 cli/             # CLI commands\n\u2514\u2500\u2500 utils/           # Utilities\n\npackages/\n\u251c\u2500\u2500 tiny_openai/     # OpenAI integration\n\u251c\u2500\u2500 tiny_anthropic/  # Anthropic integration\n\u251c\u2500\u2500 tiny_mistralai/  # Mistral integration\n\u251c\u2500\u2500 tiny_gemini/     # Gemini integration\n\u251c\u2500\u2500 tiny_brave/      # Brave search\n\u251c\u2500\u2500 tiny_chat/       # Chat UI\n\u2514\u2500\u2500 tiny_graph/      # Neo4j graphs\n</code></pre> <p>For detailed implementation, see the source code in the repository.</p>"},{"location":"examples/","title":"Examples","text":"<p>Explore practical examples of Tinygent in action.</p>"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#simple-weather-agent","title":"Simple Weather Agent","text":"<pre><code>from tinygent.tools import tool\nfrom tinygent.core.factory import build_agent\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather in a given location.\"\"\"\n    return f'The weather in {location} is sunny with a high of 75\u00b0F.'\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n)\n\nresult = agent.run('What is the weather like in Prague?')\nprint(result)\n</code></pre>"},{"location":"examples/#calculator-agent","title":"Calculator Agent","text":"<pre><code>@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[add, multiply],\n)\n\nresult = agent.run('What is (5 + 3) * 2?')\nprint(result)  # \"The result is 16\"\n</code></pre>"},{"location":"examples/#repository-examples","title":"Repository Examples","text":"<p>Tinygent includes comprehensive examples in the repository. All examples can be run with <code>uv run</code>:</p>"},{"location":"examples/#basics","title":"Basics","text":""},{"location":"examples/#1-tool-usage","title":"1. Tool Usage","text":"<p>Location: <code>examples/tool-usage/main.py</code></p> <p>Demonstrates all tool decorator types:</p> <ul> <li><code>@tool</code> - Simple tools</li> <li><code>@register_tool</code> - Global registration with caching</li> <li><code>@reasoning_tool</code> - Tools requiring reasoning</li> <li><code>@jit_tool</code> - Just-in-time code generation</li> </ul> <p>Run:</p> <pre><code>uv run examples/tool-usage/main.py\n</code></pre> <p>Highlights:</p> <pre><code># Pydantic model tools\n@register_tool(use_cache=True)\ndef add(data: AddInput) -&gt; int:\n    \"\"\"Adds two numbers together.\"\"\"\n    return data.a + data.b\n\n# Regular parameter tools\n@register_tool(use_cache=True)\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiplies two numbers together.\"\"\"\n    return a * b\n\n# Async tools\n@tool\nasync def greet(data: GreetInput) -&gt; str:\n    \"\"\"Greets a person by name.\"\"\"\n    return f'Hello, {data.name}!'\n\n# Generator tools\n@jit_tool(jit_instruction='Count from 1 to n, yielding each number.')\ndef count(data: CountInput):\n    \"\"\"Counts from 1 to n, yielding each number.\"\"\"\n    for i in range(1, data.n + 1):\n        yield i\n\n# Reasoning tools\n@register_reasoning_tool(reasoning_prompt='Explain why you are performing this search.')\ndef search(data: SearchInput) -&gt; str:\n    \"\"\"Search for something.\"\"\"\n    return f'Results for {data.query}'\n</code></pre>"},{"location":"examples/#2-llm-usage","title":"2. LLM Usage","text":"<p>Location: <code>examples/llm-usage/main.py</code></p> <p>Direct LLM usage without agents.</p> <p>Run:</p> <pre><code>uv run examples/llm-usage/main.py\n</code></pre>"},{"location":"examples/#3-function-calling","title":"3. Function Calling","text":"<p>Location: <code>examples/function-calling/main.py</code></p> <p>Demonstrates LLM function calling capabilities.</p> <p>Run:</p> <pre><code>uv run examples/function-calling/main.py\n</code></pre>"},{"location":"examples/#memory-examples","title":"Memory Examples","text":""},{"location":"examples/#1-buffer-chat-memory","title":"1. Buffer Chat Memory","text":"<p>Location: <code>examples/memory/basic-chat-memory/main.py</code></p> <p>Full conversation history with filtering.</p> <p>Run:</p> <pre><code>uv run examples/memory/basic-chat-memory/main.py\n</code></pre> <p>Highlights:</p> <pre><code>from tinygent.memory import BufferChatMemory\n\nmemory = BufferChatMemory()\n\n# Save messages\nmemory.save_context(TinyHumanMessage(content='Hello'))\nmemory.save_context(TinyChatMessage(content='Hi there!'))\n\n# Filter messages\nmemory._chat_history.add_filter(\n    'only_human',\n    lambda m: isinstance(m, TinyHumanMessage)\n)\n</code></pre>"},{"location":"examples/#2-summary-buffer-memory","title":"2. Summary Buffer Memory","text":"<p>Location: <code>examples/memory/buffer-summary-memory/main.py</code></p> <p>Automatically summarizes old messages to save tokens.</p> <p>Run:</p> <pre><code>uv run examples/memory/buffer-summary-memory/main.py\n</code></pre>"},{"location":"examples/#3-window-buffer-memory","title":"3. Window Buffer Memory","text":"<p>Location: <code>examples/memory/buffer-window-chat-memory/main.py</code></p> <p>Keeps only the last N messages.</p> <p>Run:</p> <pre><code>uv run examples/memory/buffer-window-chat-memory/main.py\n</code></pre>"},{"location":"examples/#4-combined-memory","title":"4. Combined Memory","text":"<p>Location: <code>examples/memory/combined-memory/main.py</code></p> <p>Combine multiple memory strategies.</p> <p>Run:</p> <pre><code>uv run examples/memory/combined-memory/main.py\n</code></pre>"},{"location":"examples/#agent-examples","title":"Agent Examples","text":""},{"location":"examples/#1-react-agent","title":"1. ReAct Agent","text":"<p>Location: <code>examples/agents/react/main.py</code></p> <p>Full ReAct agent with middleware tracking the thought-action-observation cycle.</p> <p>Run:</p> <pre><code>export OPENAI_API_KEY=\"your-key\"\nuv run examples/agents/react/main.py\n</code></pre> <p>Highlights:</p> <pre><code>from tinygent.agents.react_agent import TinyReActAgent\n\n# Custom middleware tracking ReAct cycles\nclass ReActCycleMiddleware(TinyBaseMiddleware):\n    def on_reasoning(self, *, run_id: str, reasoning: str) -&gt; None:\n        print(f\"THOUGHT: {reasoning}\")\n\n    def before_tool_call(self, *, run_id: str, tool, args) -&gt; None:\n        print(f\"ACTION: {tool.info.name}({args})\")\n\n    def after_tool_call(self, *, run_id: str, tool, args, result) -&gt; None:\n        print(f\"OBSERVATION: {result}\")\n\nagent = TinyReActAgent(\n    llm=build_llm('openai:gpt-4o'),\n    tools=[get_weather, get_best_destination],\n    middleware=[ReActCycleMiddleware()],\n    memory=BufferChatMemory(),\n)\n</code></pre> <p>Quick version: <code>examples/agents/react/quick.py</code></p>"},{"location":"examples/#2-multi-step-agent","title":"2. Multi-Step Agent","text":"<p>Location: <code>examples/agents/multi-step/main.py</code></p> <p>Planning agent that creates a plan before execution.</p> <p>Run:</p> <pre><code>uv run examples/agents/multi-step/main.py\n</code></pre> <p>Quick version: <code>examples/agents/multi-step/quick.py</code></p>"},{"location":"examples/#3-squad-agent","title":"3. Squad Agent","text":"<p>Location: <code>examples/agents/squad/main.py</code></p> <p>Multi-agent collaboration with specialized sub-agents.</p> <p>Run:</p> <pre><code>uv run examples/agents/squad/main.py\n</code></pre> <p>Quick version: <code>examples/agents/squad/quick.py</code></p>"},{"location":"examples/#4-map-agent","title":"4. MAP Agent","text":"<p>Location: <code>examples/agents/map/main.py</code></p> <p>Modular Agentic Planner with dynamic replanning.</p> <p>Run:</p> <pre><code>uv run examples/agents/map/main.py\n</code></pre> <p>Quick version: <code>examples/agents/map/quick.py</code></p>"},{"location":"examples/#5-middleware-examples","title":"5. Middleware Examples","text":"<p>Location: <code>examples/agents/middleware/main.py</code></p> <p>Three custom middleware examples:</p> <ol> <li>AnswerLoggingMiddleware - Logs final answers</li> <li>LLMCallTimingMiddleware - Tracks LLM call performance</li> <li>ToolCallAuditMiddleware - Audits all tool executions</li> </ol> <p>Run:</p> <pre><code>uv run examples/agents/middleware/main.py\n</code></pre> <p>Highlights:</p> <pre><code># Timing middleware\nclass LLMCallTimingMiddleware(TinyBaseMiddleware):\n    def before_llm_call(self, *, run_id: str, llm_input) -&gt; None:\n        self.call_start_times[run_id] = time.time()\n\n    def after_llm_call(self, *, run_id: str, llm_input, result) -&gt; None:\n        duration = time.time() - self.call_start_times[run_id]\n        print(f\"LLM call took {duration:.2f}s\")\n\n# Audit middleware\nclass ToolCallAuditMiddleware(TinyBaseMiddleware):\n    def after_tool_call(self, *, run_id: str, tool, args, result) -&gt; None:\n        audit_entry = {\n            'timestamp': time.time(),\n            'tool': tool.info.name,\n            'args': args,\n            'result': str(result)[:100],\n        }\n        self.audit_log.append(audit_entry)\n\nagent = TinyMultiStepAgent(\n    llm=build_llm('openai:gpt-4o'),\n    tools=[greet, add_numbers],\n    middleware=[\n        LLMCallTimingMiddleware(),\n        ToolCallAuditMiddleware(),\n        AnswerLoggingMiddleware(),\n    ],\n)\n</code></pre>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":""},{"location":"examples/#embeddings","title":"Embeddings","text":"<p>Location: <code>examples/embeddings/main.py</code></p> <p>Generate vector embeddings for semantic search.</p> <p>Run:</p> <pre><code>uv sync --extra openai --extra voyageai\nuv run examples/embeddings/main.py\n</code></pre>"},{"location":"examples/#cross-encoder","title":"Cross-Encoder","text":"<p>Location: <code>examples/cross-encoder/main.py</code></p> <p>Re-rank search results using cross-encoders.</p> <p>Run:</p> <pre><code>uv run examples/cross-encoder/main.py\n</code></pre>"},{"location":"examples/#knowledge-graph","title":"Knowledge Graph","text":"<p>Location: <code>examples/knowledge-graph/main.py</code></p> <p>Build knowledge graphs with Neo4j.</p> <p>Run:</p> <pre><code>uv sync --extra tiny_graph\n# Requires Neo4j running\nuv run examples/knowledge-graph/main.py\n</code></pre>"},{"location":"examples/#tracing","title":"Tracing","text":"<p>Location: <code>examples/tracing/main.py</code></p> <p>Advanced tracing and observability.</p> <p>Run:</p> <pre><code>uv run examples/tracing/main.py\n</code></pre>"},{"location":"examples/#chat-app","title":"Chat App","text":"<p>Location: <code>examples/chat-app/main.py</code></p> <p>Full chat application with FastAPI.</p> <p>Run:</p> <pre><code>uv sync --extra tiny_chat\nuv run examples/chat-app/main.py\n</code></pre> <p>Then open: <code>http://localhost:8000</code></p>"},{"location":"examples/#package-examples","title":"Package Examples","text":""},{"location":"examples/#brave-search","title":"Brave Search","text":"<p>Location: <code>packages/tiny_brave/</code></p> <p>Real-world search tool using Brave Search API.</p> <pre><code># Install\nuv sync --extra tiny_brave\n\n# Use\nfrom tiny_brave import brave_search\n\n@register_tool\ndef web_search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    return brave_search(query)\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[web_search]\n)\n</code></pre>"},{"location":"examples/#tiny-chat","title":"Tiny Chat","text":"<p>Location: <code>packages/tiny_chat/</code></p> <p>Web-based chat interface for agents.</p> <pre><code>uv sync --extra tiny_chat\nuv run -m tiny_chat\n</code></pre>"},{"location":"examples/#tiny-graph","title":"Tiny Graph","text":"<p>Location: <code>packages/tiny_graph/</code></p> <p>Knowledge graph integration with Neo4j.</p> <pre><code>uv sync --extra tiny_graph\n</code></pre>"},{"location":"examples/#running-examples","title":"Running Examples","text":""},{"location":"examples/#prerequisites","title":"Prerequisites","text":"<pre><code># Clone repository\ngit clone git@github.com:filchy/tinygent.git\ncd tinygent\n\n# Setup environment\nuv venv --seed .venv\nsource .venv/bin/activate\n\n# Install dependencies\nuv sync --extra openai\n</code></pre>"},{"location":"examples/#set-api-keys","title":"Set API Keys","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport BRAVE_API_KEY=\"...\"\n</code></pre>"},{"location":"examples/#run-any-example","title":"Run Any Example","text":"<pre><code>uv run examples/agents/react/main.py\nuv run examples/tool-usage/main.py\nuv run examples/memory/basic-chat-memory/main.py\n</code></pre>"},{"location":"examples/#example-patterns","title":"Example Patterns","text":""},{"location":"examples/#pattern-1-development-workflow","title":"Pattern 1: Development Workflow","text":"<pre><code># 1. Start with simple example\nuv run examples/tool-usage/main.py\n\n# 2. Try agent example\nuv run examples/agents/react/quick.py\n\n# 3. Add memory\nuv run examples/memory/basic-chat-memory/main.py\n\n# 4. Add middleware\nuv run examples/agents/middleware/main.py\n\n# 5. Build your own!\n</code></pre>"},{"location":"examples/#pattern-2-learning-path","title":"Pattern 2: Learning Path","text":"<ol> <li>Basics \u2192 Tool usage, LLM usage</li> <li>Agents \u2192 ReAct agent, Multi-step agent</li> <li>Memory \u2192 Buffer memory, Window memory</li> <li>Advanced \u2192 Middleware, Squad agents, MAP agents</li> <li>Production \u2192 Chat app, Knowledge graphs, Tracing</li> </ol>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a cool example? Contribute it!</p> <ol> <li>Create <code>examples/your-example/main.py</code></li> <li>Add README explaining the example</li> <li>Submit a pull request</li> </ol>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started: Install and setup</li> <li>Core Concepts: Learn the fundamentals</li> <li>Guides: Build your own agents</li> </ul>"},{"location":"examples/#example-code-structure","title":"Example Code Structure","text":"<p>All examples follow this structure:</p> <pre><code>examples/\n\u251c\u2500\u2500 agents/\n\u2502   \u251c\u2500\u2500 react/\n\u2502   \u2502   \u251c\u2500\u2500 main.py          # Full example with middleware\n\u2502   \u2502   \u251c\u2500\u2500 quick.py         # Minimal example\n\u2502   \u2502   \u2514\u2500\u2500 prompts.yaml     # Custom prompts\n\u2502   \u251c\u2500\u2500 multi-step/\n\u2502   \u251c\u2500\u2500 squad/\n\u2502   \u2514\u2500\u2500 map/\n\u251c\u2500\u2500 memory/\n\u2502   \u251c\u2500\u2500 basic-chat-memory/\n\u2502   \u251c\u2500\u2500 buffer-summary-memory/\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 tool-usage/\n\u251c\u2500\u2500 llm-usage/\n\u2514\u2500\u2500 ...\n</code></pre> <p>Each example is self-contained and can be run independently.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you install Tinygent and create your first agent.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Python 3.10+ installed</li> <li>Git for cloning the repository</li> <li>uv - Modern Python package manager</li> </ul>"},{"location":"getting-started/#installing-uv","title":"Installing uv","text":"<pre><code># macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Via pip\npip install uv\n</code></pre>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#from-source","title":"From Source","text":"<ol> <li>Clone the repository</li> </ol> <pre><code>git clone git@github.com:filchy/tinygent.git\ncd tinygent\n</code></pre> <ol> <li>Create a virtual environment</li> </ol> <pre><code>uv venv --seed .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre> <ol> <li>Install Tinygent</li> </ol> <p>Option A: Core only (minimal installation)</p> <pre><code>uv sync\n</code></pre> <p>Option B: With specific providers</p> <pre><code># OpenAI\nuv sync --extra openai\n\n# Anthropic\nuv sync --extra anthropic\n\n# Multiple providers\nuv sync --extra openai --extra anthropic --extra mistralai\n</code></pre> <p>Option C: Everything (including dev tools and all packages)</p> <pre><code>uv sync --all-groups --all-extras\n</code></pre> <ol> <li>Install in editable mode (for development)</li> </ol> <pre><code>uv pip install -e .\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":""},{"location":"getting-started/#api-keys","title":"API Keys","text":"<p>Tinygent uses environment variables for API keys. Set them before running your code:</p> <pre><code># OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Mistral AI\nexport MISTRAL_API_KEY=\"...\"\n\n# Google Gemini\nexport GEMINI_API_KEY=\"...\"\n\n# VoyageAI (embeddings)\nexport VOYAGEAI_API_KEY=\"...\"\n\n# Brave Search\nexport BRAVE_API_KEY=\"...\"\n</code></pre> <p>Pro Tip: Create a <code>.env</code> file in your project root:</p> <pre><code># .env\nOPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>Then load it in your code:</p> <pre><code>from dotenv import load_dotenv\nload_dotenv()\n</code></pre>"},{"location":"getting-started/#your-first-agent","title":"Your First Agent","text":"<p>Let's build a simple weather assistant.</p>"},{"location":"getting-started/#step-1-create-a-tool","title":"Step 1: Create a Tool","text":"<p>Tools are functions that agents can call. Use the <code>@tool</code> decorator:</p> <pre><code>from tinygent.tools import tool\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city or location to get weather for\n\n    Returns:\n        A string describing the current weather\n    \"\"\"\n    # In a real app, you'd call a weather API\n    return f'The weather in {location} is sunny with a high of 75\u00b0F.'\n</code></pre>"},{"location":"getting-started/#step-2-build-an-agent","title":"Step 2: Build an Agent","text":"<p>Use the factory function to create a ReAct agent:</p> <pre><code>from tinygent.core.factory import build_agent\n\nagent = build_agent(\n    'react',                    # Agent type\n    llm='openai:gpt-4o-mini',  # LLM provider:model\n    tools=[get_weather],        # List of tools\n    max_iterations=5,           # Max reasoning loops\n)\n</code></pre>"},{"location":"getting-started/#step-3-run-the-agent","title":"Step 3: Run the Agent","text":"<pre><code># Synchronous\nresult = agent.run('What is the weather like in Prague?')\nprint(result)\n\n# Asynchronous with streaming\nimport asyncio\n\nasync def main():\n    async for chunk in agent.run_stream('What is the weather in Prague?'):\n        print(chunk, end='', flush=True)\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#complete-example","title":"Complete Example","text":"<pre><code># weather_agent.py\nfrom tinygent.tools import tool\nfrom tinygent.core.factory import build_agent\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather in a given location.\"\"\"\n    return f'The weather in {location} is sunny with a high of 75\u00b0F.'\n\n@tool\ndef get_forecast(location: str, days: int = 3) -&gt; str:\n    \"\"\"Get the weather forecast for the next few days.\"\"\"\n    return f'{days}-day forecast for {location}: Sunny, then partly cloudy.'\n\ndef main():\n    agent = build_agent(\n        'react',\n        llm='openai:gpt-4o-mini',\n        tools=[get_weather, get_forecast],\n    )\n\n    result = agent.run(\n        'What is the weather in Prague? Also give me a 5-day forecast.'\n    )\n    print(result)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Run it:</p> <pre><code>export OPENAI_API_KEY=\"your-key\"\nuv run weather_agent.py\n</code></pre>"},{"location":"getting-started/#understanding-the-output","title":"Understanding the Output","text":"<p>When you run the agent, you'll see the ReAct reasoning cycle:</p> <ol> <li>Thought: Agent reasons about what to do</li> <li>Action: Agent decides to call a tool</li> <li>Observation: Tool returns a result</li> <li>Repeat: Until agent has enough information</li> <li>Final Answer: Agent provides the answer to the user</li> </ol>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have a working agent, explore more concepts:</p> <ul> <li>Agents: Learn about different agent types (ReAct, MultiStep, Squad, MAP)</li> <li>Tools: Discover <code>@reasoning_tool</code> and <code>@jit_tool</code></li> <li>Memory: Add conversation memory to your agents</li> <li>LLMs: Use different LLM providers</li> <li>Middleware: Customize agent behavior with hooks</li> </ul>"},{"location":"getting-started/#running-examples","title":"Running Examples","text":"<p>Tinygent includes many examples in the <code>examples/</code> directory:</p> <pre><code># ReAct agent\nuv run examples/agents/react/main.py\n\n# Multi-step agent\nuv run examples/agents/multi-step/main.py\n\n# Memory examples\nuv run examples/memory/basic-chat-memory/main.py\n\n# Tool usage\nuv run examples/tool-usage/main.py\n</code></pre> <p>Explore the examples to see advanced patterns and use cases.</p>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#import-errors","title":"Import Errors","text":"<p>If you get import errors, ensure you've installed the package:</p> <pre><code>uv pip install -e .\n</code></pre>"},{"location":"getting-started/#missing-dependencies","title":"Missing Dependencies","text":"<p>Install the provider you need:</p> <pre><code>uv sync --extra openai\n</code></pre>"},{"location":"getting-started/#api-key-issues","title":"API Key Issues","text":"<p>Verify your environment variables are set:</p> <pre><code>echo $OPENAI_API_KEY\n</code></pre>"},{"location":"getting-started/#development-setup","title":"Development Setup","text":"<p>For contributors and advanced users:</p> <pre><code># Install with all dev dependencies\nuv sync --all-groups --all-extras\n\n# Format code\nuv run fmt\n\n# Run linter and type checker\nuv run lint\n\n# Run tests (if available)\npytest\n</code></pre>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<ul> <li> <p> Learn Core Concepts</p> <p>Understand agents, tools, and memory</p> <p> Core Concepts</p> </li> <li> <p> Build Custom Tools</p> <p>Create powerful custom tools for your agents</p> <p> Tool Guide</p> </li> <li> <p> Advanced Agents</p> <p>Build multi-agent systems and complex workflows</p> <p> Agent Guide</p> </li> </ul>"},{"location":"concepts/agents/","title":"Agents","text":"<p>Agents are the core of Tinygent. They combine LLMs, tools, and reasoning to accomplish tasks autonomously.</p>"},{"location":"concepts/agents/#what-is-an-agent","title":"What is an Agent?","text":"<p>An agent is an autonomous system that:</p> <ol> <li>Receives a task or question from the user</li> <li>Reasons about how to solve it</li> <li>Uses tools to gather information or take actions</li> <li>Iterates until it has enough information</li> <li>Provides a final answer</li> </ol> <p>Unlike simple LLM calls, agents can:</p> <ul> <li>Break down complex tasks into steps</li> <li>Call external tools and APIs</li> <li>Remember conversation history</li> <li>Retry and self-correct</li> <li>Coordinate with other agents</li> </ul>"},{"location":"concepts/agents/#agent-types","title":"Agent Types","text":"<p>Tinygent provides 4 built-in agent types:</p>"},{"location":"concepts/agents/#1-react-agent","title":"1. ReAct Agent","text":"<p>Best for: General-purpose tasks, tool-calling, reasoning loops</p> <p>The ReAct (Reasoning + Acting) agent follows a thought-action-observation cycle:</p> <pre><code>Thought: I need to find the weather in Prague\nAction: get_weather(location=\"Prague\")\nObservation: The weather is sunny, 75\u00b0F\nThought: I have the information I need\nFinal Answer: The weather in Prague is sunny with a high of 75\u00b0F.\n</code></pre> <p>Usage:</p> <pre><code>from tinygent.core.factory import build_agent\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather, search_web],\n    max_iterations=5,  # Max reasoning cycles\n)\n\nresult = agent.run('What is the weather in Prague?')\n</code></pre> <p>When to use:</p> <ul> <li>Single-task execution</li> <li>Tool-heavy workflows</li> <li>When you need transparent reasoning</li> </ul> <p>Middleware Hooks Activated:</p> <ul> <li><code>before_llm_call</code> / <code>after_llm_call</code> - For LLM calls</li> <li><code>before_tool_call</code> / <code>after_tool_call</code> - For tool executions</li> <li><code>on_tool_reasoning</code> - When reasoning tools generate reasoning</li> <li><code>on_answer</code> / <code>on_answer_chunk</code> - For final answers</li> <li><code>on_error</code> - On any error</li> </ul> <p>Note: ReAct agent does not use <code>on_plan</code> or <code>on_reasoning</code> hooks.</p>"},{"location":"concepts/agents/#2-multistep-agent","title":"2. MultiStep Agent","text":"<p>Best for: Complex tasks requiring planning, step-by-step execution</p> <p>The MultiStep agent creates a plan first, then executes it step by step:</p> <pre><code>1. Create Plan:\n   - Step 1: Get weather in Prague\n   - Step 2: Find best restaurant\n   - Step 3: Make recommendation\n\n2. Execute Steps:\n   - Execute Step 1 \u2192 Result A\n   - Execute Step 2 \u2192 Result B\n   - Execute Step 3 \u2192 Result C\n\n3. Synthesize Final Answer\n</code></pre> <p>Usage:</p> <pre><code>from tinygent.agents.multi_step_agent import TinyMultiStepAgent\nfrom tinygent.core.factory import build_llm\n\nagent = TinyMultiStepAgent(\n    llm=build_llm('openai:gpt-4o'),\n    tools=[get_weather, find_restaurants, book_table],\n    max_iterations=10,\n)\n\nresult = agent.run(\n    'Plan a dinner in Prague tonight - check weather, '\n    'find a good restaurant, and book a table.'\n)\n</code></pre> <p>When to use:</p> <ul> <li>Multi-step workflows</li> <li>Tasks requiring explicit planning</li> <li>When you need to see the plan before execution</li> </ul> <p>Middleware Hooks Activated:</p> <ul> <li><code>before_llm_call</code> / <code>after_llm_call</code> - For LLM calls</li> <li><code>before_tool_call</code> / <code>after_tool_call</code> - For tool executions</li> <li><code>on_plan</code> - When creating initial or updated plan</li> <li><code>on_reasoning</code> - For agent reasoning steps</li> <li><code>on_tool_reasoning</code> - When reasoning tools generate reasoning</li> <li><code>on_answer</code> / <code>on_answer_chunk</code> - For final answers</li> <li><code>on_error</code> - On any error</li> </ul>"},{"location":"concepts/agents/#3-squad-agent","title":"3. Squad Agent","text":"<p>Best for: Multi-agent collaboration, specialized roles</p> <p>Squad agents coordinate multiple sub-agents, each with specialized capabilities:</p> <pre><code>Coordinator Agent\n\u251c\u2500\u2500 Research Agent (tools: web_search, wikipedia)\n\u251c\u2500\u2500 Analysis Agent (tools: calculator, data_analyzer)\n\u2514\u2500\u2500 Writer Agent (tools: text_formatter, summarizer)\n</code></pre> <p>Usage:</p> <pre><code>from tinygent.agents.squad_agent import TinySquadAgent\nfrom tinygent.core.factory import build_agent\n\n# Create specialized agents\nresearcher = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[web_search, wikipedia],\n)\n\nanalyst = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[calculator, analyze_data],\n)\n\n# Create squad\nsquad = TinySquadAgent(\n    llm=build_llm('openai:gpt-4o'),\n    agents=[researcher, analyst],\n    max_iterations=5,\n)\n\nresult = squad.run(\n    'Research the GDP of Czech Republic and compare it to Poland'\n)\n</code></pre> <p>When to use:</p> <ul> <li>Tasks requiring different specializations</li> <li>Divide-and-conquer strategies</li> <li>When you want parallel execution</li> </ul> <p>Middleware Hooks Activated:</p> <ul> <li><code>before_llm_call</code> / <code>after_llm_call</code> - For LLM calls (delegated to sub-agents)</li> <li><code>before_tool_call</code> / <code>after_tool_call</code> - For tool executions (delegated to sub-agents)</li> <li><code>on_answer</code> / <code>on_answer_chunk</code> - For final aggregated answers</li> <li><code>on_error</code> - On any error</li> </ul> <p>Note: Squad agent delegates most hooks to its sub-agents. Hook activation depends on sub-agent types.</p>"},{"location":"concepts/agents/#4-map-agent-modular-agentic-planner","title":"4. MAP Agent (Modular Agentic Planner)","text":"<p>Best for: Complex workflows with dynamic replanning</p> <p>MAP agents create modular plans and can adapt them based on execution results:</p> <pre><code>1. Initial Plan:\n   Module A \u2192 Module B \u2192 Module C\n\n2. Execute Module A\n   \u2192 Result changes plan\n\n3. Updated Plan:\n   Module A \u2192 Module D \u2192 Module B \u2192 Module C\n\n4. Continue execution with new plan\n</code></pre> <p>Usage:</p> <pre><code>from tinygent.agents.map_agent import TinyMAPAgent\n\nagent = TinyMAPAgent(\n    llm=build_llm('openai:gpt-4o'),\n    tools=[...],\n    max_iterations=15,\n)\n\nresult = agent.run('Complex multi-step task with potential replanning')\n</code></pre> <p>When to use:</p> <ul> <li>Highly dynamic tasks</li> <li>When the plan may need adjustment</li> <li>Research and exploration tasks</li> </ul> <p>Middleware Hooks Activated:</p> <ul> <li><code>before_llm_call</code> / <code>after_llm_call</code> - For LLM calls</li> <li><code>before_tool_call</code> / <code>after_tool_call</code> - For tool executions</li> <li><code>on_plan</code> - When creating search/action plans</li> <li><code>on_answer</code> / <code>on_answer_chunk</code> - For final answers</li> <li><code>on_error</code> - On any error</li> </ul> <p>Note: MAP agent uses <code>on_plan</code> for action summaries but not <code>on_reasoning</code> or <code>on_tool_reasoning</code>.</p>"},{"location":"concepts/agents/#agent-configuration","title":"Agent Configuration","text":"<p>All agents support common configuration options:</p>"},{"location":"concepts/agents/#using-config-objects","title":"Using Config Objects","text":"<pre><code>from tinygent.agents.react_agent import ReactAgentConfig\nfrom tinygent.memory import BufferChatMemory\n\nconfig = ReactAgentConfig(\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n    max_iterations=10,\n    memory=BufferChatMemory(),\n    temperature=0.7,\n    stop_sequences=['STOP', 'END'],\n)\n\nagent = config.build()\n</code></pre>"},{"location":"concepts/agents/#using-factory-functions","title":"Using Factory Functions","text":"<pre><code>from tinygent.core.factory import build_agent\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n    max_iterations=10,\n)\n</code></pre>"},{"location":"concepts/agents/#memory","title":"Memory","text":"<p>Agents can remember conversation history using memory:</p> <pre><code>from tinygent.memory import BufferChatMemory\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n    memory=BufferChatMemory(),\n)\n\n# First conversation\nagent.run('What is the weather in Prague?')\n\n# Second conversation - agent remembers context\nagent.run('How about tomorrow?')\n</code></pre> <p>See Memory for more details.</p>"},{"location":"concepts/agents/#middleware","title":"Middleware","text":"<p>Customize agent behavior with middleware hooks:</p> <pre><code>from tinygent.agents.middleware import TinyBaseMiddleware\n\nclass LoggingMiddleware(TinyBaseMiddleware):\n    def on_reasoning(self, *, run_id: str, reasoning: str) -&gt; None:\n        print(f\"[THOUGHT] {reasoning}\")\n\n    def before_tool_call(self, *, run_id: str, tool, args) -&gt; None:\n        print(f\"[ACTION] {tool.info.name}({args})\")\n\n    def after_tool_call(self, *, run_id: str, tool, args, result) -&gt; None:\n        print(f\"[OBSERVATION] {result}\")\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n    middleware=[LoggingMiddleware()],\n)\n</code></pre> <p>See Middleware for more details.</p>"},{"location":"concepts/agents/#streaming-responses","title":"Streaming Responses","text":"<p>All agents support async streaming:</p> <pre><code>import asyncio\n\nasync def main():\n    agent = build_agent('react', llm='openai:gpt-4o-mini', tools=[...])\n\n    # Stream tokens as they're generated\n    async for chunk in agent.run_stream('What is the weather?'):\n        print(chunk, end='', flush=True)\n\nasyncio.run(main())\n</code></pre>"},{"location":"concepts/agents/#choosing-the-right-agent","title":"Choosing the Right Agent","text":"Task Type Recommended Agent Why Single API call ReAct Simple, fast, transparent Multi-step workflow MultiStep Explicit planning Requires specialists Squad Divide and conquer Dynamic/exploratory MAP Adaptive replanning General chatbot ReAct + Memory Conversational Research task MAP or Squad Flexible exploration"},{"location":"concepts/agents/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"concepts/agents/#custom-prompts","title":"Custom Prompts","text":"<p>Override default prompts:</p> <pre><code>from tinygent.agents.react_agent import ReActPromptTemplate\n\ncustom_prompt = ReActPromptTemplate(\n    system=\"You are a helpful AI assistant specialized in weather.\",\n    user_prefix=\"Human: \",\n    assistant_prefix=\"Assistant: \",\n    thought_prefix=\"Thinking: \",\n    action_prefix=\"Action: \",\n    observation_prefix=\"Observation: \",\n)\n\nagent = TinyReActAgent(\n    llm=build_llm('openai:gpt-4o-mini'),\n    tools=[get_weather],\n    prompt_template=custom_prompt,\n)\n</code></pre>"},{"location":"concepts/agents/#error-handling","title":"Error Handling","text":"<p>Agents automatically handle tool errors:</p> <pre><code>@tool\ndef risky_operation(data: str) -&gt; str:\n    \"\"\"An operation that might fail.\"\"\"\n    if not data:\n        raise ValueError(\"Data cannot be empty\")\n    return f\"Processed: {data}\"\n\n# Agent will catch errors and try alternative approaches\nagent = build_agent('react', llm='openai:gpt-4o-mini', tools=[risky_operation])\nresult = agent.run('Process this data')  # Handles errors gracefully\n</code></pre>"},{"location":"concepts/agents/#next-steps","title":"Next Steps","text":"<ul> <li>Tools: Learn about tool types and decorators</li> <li>Memory: Add conversation memory</li> <li>Middleware: Customize agent behavior</li> <li>Building Agents Guide: Step-by-step guide</li> </ul>"},{"location":"concepts/agents/#examples","title":"Examples","text":"<p>Check out the examples directory:</p> <ul> <li><code>examples/agents/react/main.py</code> - ReAct agent with tools</li> <li><code>examples/agents/multi-step/main.py</code> - Multi-step planning</li> <li><code>examples/agents/squad/main.py</code> - Multi-agent collaboration</li> <li><code>examples/agents/map/main.py</code> - MAP agent with replanning</li> <li><code>examples/agents/middleware/main.py</code> - Custom middleware</li> </ul>"},{"location":"concepts/llms/","title":"LLMs (Language Models)","text":"<p>Tinygent supports multiple LLM providers with a unified interface. Switch between OpenAI, Anthropic, Mistral, and Gemini without changing your code.</p>"},{"location":"concepts/llms/#provider-string-format","title":"Provider String Format","text":"<p>All LLMs in Tinygent use the format:</p> <pre><code>provider:model\n</code></pre> <p>Examples:</p> <pre><code># OpenAI\nllm = build_llm('openai:gpt-4o')\nllm = build_llm('openai:gpt-4o-mini')\nllm = build_llm('openai:gpt-3.5-turbo')\n\n# Anthropic Claude\nllm = build_llm('anthropic:claude-3-5-sonnet')\nllm = build_llm('anthropic:claude-3-5-haiku')\nllm = build_llm('anthropic:claude-3-opus')\n\n# Mistral AI\nllm = build_llm('mistralai:mistral-large-latest')\nllm = build_llm('mistralai:mistral-small-latest')\n\n# Google Gemini\nllm = build_llm('gemini:gemini-2.0-flash-exp')\nllm = build_llm('gemini:gemini-pro')\n</code></pre>"},{"location":"concepts/llms/#supported-providers","title":"Supported Providers","text":""},{"location":"concepts/llms/#openai","title":"OpenAI","text":"<p>Installation:</p> <pre><code>uv sync --extra openai\n</code></pre> <p>Environment Variable:</p> <pre><code>export OPENAI_API_KEY=\"sk-...\"\n</code></pre> <p>Available Models:</p> <ul> <li><code>gpt-4o</code> - Latest GPT-4 Optimized</li> <li><code>gpt-4o-mini</code> - Smaller, faster GPT-4</li> <li><code>gpt-3.5-turbo</code> - Fast and cost-effective</li> <li><code>gpt-4-turbo</code> - Previous generation</li> </ul> <p>Usage:</p> <pre><code>from tinygent.core.factory import build_llm\n\nllm = build_llm('openai:gpt-4o-mini', temperature=0.7)\n\n# Direct call\nresponse = llm.generate(\"What is AI?\")\nprint(response.content)\n\n# With streaming\nasync for chunk in llm.stream(\"Tell me a story\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"concepts/llms/#anthropic-claude","title":"Anthropic Claude","text":"<p>Installation:</p> <pre><code>uv sync --extra anthropic\n</code></pre> <p>Environment Variable:</p> <pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre> <p>Available Models:</p> <ul> <li><code>claude-3-5-sonnet</code> - Best overall performance</li> <li><code>claude-3-5-haiku</code> - Fast and efficient</li> <li><code>claude-3-opus</code> - Most capable (expensive)</li> </ul> <p>Usage:</p> <pre><code>llm = build_llm('anthropic:claude-3-5-sonnet', temperature=0.5)\n\nresponse = llm.generate(\"Explain quantum computing\")\nprint(response.content)\n</code></pre>"},{"location":"concepts/llms/#mistral-ai","title":"Mistral AI","text":"<p>Installation:</p> <pre><code>uv sync --extra mistralai\n</code></pre> <p>Environment Variable:</p> <pre><code>export MISTRAL_API_KEY=\"...\"\n</code></pre> <p>Available Models:</p> <ul> <li><code>mistral-large-latest</code> - Most capable</li> <li><code>mistral-small-latest</code> - Fast and efficient</li> <li><code>open-mistral-7b</code> - Open source</li> </ul> <p>Usage:</p> <pre><code>llm = build_llm('mistralai:mistral-large-latest')\n\nresponse = llm.generate(\"What is machine learning?\")\nprint(response.content)\n</code></pre>"},{"location":"concepts/llms/#google-gemini","title":"Google Gemini","text":"<p>Installation:</p> <pre><code>uv sync --extra gemini\n</code></pre> <p>Environment Variable:</p> <pre><code>export GEMINI_API_KEY=\"...\"\n</code></pre> <p>Available Models:</p> <ul> <li><code>gemini-2.0-flash-exp</code> - Latest Flash model</li> <li><code>gemini-pro</code> - Production model</li> <li><code>gemini-ultra</code> - Most capable (limited access)</li> </ul> <p>Usage:</p> <pre><code>llm = build_llm('gemini:gemini-2.0-flash-exp')\n\nresponse = llm.generate(\"Explain neural networks\")\nprint(response.content)\n</code></pre>"},{"location":"concepts/llms/#configuration-options","title":"Configuration Options","text":""},{"location":"concepts/llms/#temperature","title":"Temperature","text":"<p>Controls randomness (0.0 = deterministic, 2.0 = very random):</p> <pre><code># Deterministic (good for factual tasks)\nllm = build_llm('openai:gpt-4o-mini', temperature=0.0)\n\n# Balanced (default)\nllm = build_llm('openai:gpt-4o-mini', temperature=0.7)\n\n# Creative (good for storytelling)\nllm = build_llm('openai:gpt-4o-mini', temperature=1.5)\n</code></pre>"},{"location":"concepts/llms/#max-tokens","title":"Max Tokens","text":"<p>Limit response length:</p> <pre><code>llm = build_llm('openai:gpt-4o-mini', max_tokens=500)\n\n# Response will be truncated at ~500 tokens\nresponse = llm.generate(\"Write a long essay about AI\")\n</code></pre>"},{"location":"concepts/llms/#stop-sequences","title":"Stop Sequences","text":"<p>Stop generation at specific strings:</p> <pre><code>llm = build_llm(\n    'openai:gpt-4o-mini',\n    stop_sequences=['END', '\\n\\n\\n', '---']\n)\n\n# Generation stops when any stop sequence is encountered\nresponse = llm.generate(\"List items:\\n1. \")\n</code></pre>"},{"location":"concepts/llms/#top-p-nucleus-sampling","title":"Top P (Nucleus Sampling)","text":"<p>Alternative to temperature for controlling randomness:</p> <pre><code>llm = build_llm('openai:gpt-4o-mini', top_p=0.9)\n</code></pre>"},{"location":"concepts/llms/#using-llms-with-agents","title":"Using LLMs with Agents","text":""},{"location":"concepts/llms/#simple-agent","title":"Simple Agent","text":"<pre><code>from tinygent.core.factory import build_agent\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',  # Simple string\n    tools=[...],\n)\n</code></pre>"},{"location":"concepts/llms/#advanced-agent","title":"Advanced Agent","text":"<pre><code>from tinygent.agents.react_agent import TinyReActAgent\nfrom tinygent.core.factory import build_llm\n\nllm = build_llm(\n    'openai:gpt-4o',\n    temperature=0.3,\n    max_tokens=2000,\n)\n\nagent = TinyReActAgent(\n    llm=llm,  # Pre-configured LLM object\n    tools=[...],\n)\n</code></pre>"},{"location":"concepts/llms/#direct-llm-usage","title":"Direct LLM Usage","text":"<p>Use LLMs without agents:</p>"},{"location":"concepts/llms/#synchronous","title":"Synchronous","text":"<pre><code>from tinygent.core.factory import build_llm\n\nllm = build_llm('openai:gpt-4o-mini')\n\n# Simple generation\nresponse = llm.generate(\"What is 2 + 2?\")\nprint(response.content)  # \"2 + 2 equals 4.\"\n\n# With system prompt\nresponse = llm.generate(\n    \"What is the weather?\",\n    system_prompt=\"You are a helpful weather assistant.\"\n)\n</code></pre>"},{"location":"concepts/llms/#asynchronous","title":"Asynchronous","text":"<pre><code>import asyncio\n\nasync def main():\n    llm = build_llm('openai:gpt-4o-mini')\n\n    # Async generation\n    response = await llm.agenerate(\"Tell me about AI\")\n    print(response.content)\n\nasyncio.run(main())\n</code></pre>"},{"location":"concepts/llms/#streaming","title":"Streaming","text":"<pre><code>import asyncio\n\nasync def main():\n    llm = build_llm('openai:gpt-4o-mini')\n\n    # Stream tokens\n    async for chunk in llm.stream(\"Write a short poem\"):\n        print(chunk, end='', flush=True)\n\nasyncio.run(main())\n</code></pre>"},{"location":"concepts/llms/#function-calling","title":"Function Calling","text":"<p>LLMs can call functions (tools):</p> <pre><code>from tinygent.core.factory import build_llm\nfrom tinygent.tools import tool\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get weather for a location.\"\"\"\n    return f\"Sunny in {location}\"\n\nllm = build_llm('openai:gpt-4o-mini')\n\n# LLM can decide to call the function\nresponse = llm.generate(\n    \"What's the weather in Paris?\",\n    tools=[get_weather]\n)\n\n# Response includes function call\nif response.tool_calls:\n    for call in response.tool_calls:\n        print(f\"Function: {call.name}\")\n        print(f\"Arguments: {call.arguments}\")\n</code></pre>"},{"location":"concepts/llms/#cost-comparison","title":"Cost Comparison","text":"<p>Approximate costs per 1M tokens (as of 2025):</p> Provider Model Input Output OpenAI gpt-4o-mini $0.15 $0.60 OpenAI gpt-4o $2.50 $10.00 Anthropic claude-3-5-haiku $0.25 $1.25 Anthropic claude-3-5-sonnet $3.00 $15.00 Mistral mistral-large-latest $2.00 $6.00 Gemini gemini-2.0-flash-exp $0.10 $0.40 <p>Tip: Use mini/haiku models for development, upgrade for production.</p>"},{"location":"concepts/llms/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"concepts/llms/#for-development","title":"For Development","text":"<ul> <li>OpenAI: <code>gpt-4o-mini</code> - Fast, cheap, good quality</li> <li>Anthropic: <code>claude-3-5-haiku</code> - Fast, efficient</li> <li>Gemini: <code>gemini-2.0-flash-exp</code> - Cheapest option</li> </ul>"},{"location":"concepts/llms/#for-production","title":"For Production","text":"<ul> <li>OpenAI: <code>gpt-4o</code> - Excellent all-around</li> <li>Anthropic: <code>claude-3-5-sonnet</code> - Best reasoning</li> <li>Mistral: <code>mistral-large-latest</code> - European option</li> </ul>"},{"location":"concepts/llms/#for-complex-reasoning","title":"For Complex Reasoning","text":"<ul> <li>Anthropic: <code>claude-3-opus</code> - Most capable</li> <li>OpenAI: <code>gpt-4-turbo</code> - Strong reasoning</li> </ul>"},{"location":"concepts/llms/#switching-providers","title":"Switching Providers","text":"<p>Tinygent makes it trivial to switch:</p> <pre><code># Try different models for the same task\nmodels = [\n    'openai:gpt-4o-mini',\n    'anthropic:claude-3-5-haiku',\n    'mistralai:mistral-large-latest',\n    'gemini:gemini-2.0-flash-exp',\n]\n\nfor model in models:\n    agent = build_agent('react', llm=model, tools=[...])\n    result = agent.run('What is AI?')\n    print(f\"{model}: {result}\")\n</code></pre>"},{"location":"concepts/llms/#custom-llm-providers","title":"Custom LLM Providers","text":"<p>Register custom LLM providers:</p> <pre><code>from tinygent.core.runtime.global_registry import register_llm\nfrom tinygent.llms.base import BaseLLM\n\n@register_llm('custom')\nclass CustomLLM(BaseLLM):\n    def __init__(self, model: str, **kwargs):\n        super().__init__(model, **kwargs)\n\n    async def agenerate(self, prompt: str, **kwargs):\n        # Your custom implementation\n        return response\n\n# Use it\nllm = build_llm('custom:my-model')\n</code></pre>"},{"location":"concepts/llms/#embeddings","title":"Embeddings","text":"<p>For vector embeddings (RAG, semantic search):</p> <pre><code>from tinygent.core.factory import build_embedder\n\n# OpenAI embeddings\nembedder = build_embedder('openai:text-embedding-3-small')\n\n# VoyageAI embeddings\nembedder = build_embedder('voyageai:voyage-2')\n\n# Generate embeddings\nvectors = embedder.embed_documents(['Hello', 'World'])\nprint(len(vectors[0]))  # 1536 dimensions\n</code></pre> <p>Available Embedders:</p> <ul> <li><code>openai:text-embedding-3-small</code> (1536 dims)</li> <li><code>openai:text-embedding-3-large</code> (3072 dims)</li> <li><code>voyageai:voyage-2</code> (1024 dims)</li> </ul>"},{"location":"concepts/llms/#best-practices","title":"Best Practices","text":""},{"location":"concepts/llms/#1-use-environment-variables","title":"1. Use Environment Variables","text":"<pre><code># Bad - Hardcoded keys\nllm = build_llm('openai:gpt-4o', api_key='sk-...')\n\n# Good - Environment variables\nexport OPENAI_API_KEY=\"sk-...\"\nllm = build_llm('openai:gpt-4o')\n</code></pre>"},{"location":"concepts/llms/#2-start-small","title":"2. Start Small","text":"<pre><code># Development: Use cheap models\ndev_llm = build_llm('openai:gpt-4o-mini')\n\n# Production: Upgrade when needed\nprod_llm = build_llm('openai:gpt-4o')\n</code></pre>"},{"location":"concepts/llms/#3-cache-results","title":"3. Cache Results","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef cached_llm_call(prompt: str) -&gt; str:\n    llm = build_llm('openai:gpt-4o-mini')\n    return llm.generate(prompt).content\n\n# Repeated calls use cache\nresult1 = cached_llm_call(\"What is AI?\")\nresult2 = cached_llm_call(\"What is AI?\")  # Instant, no API call\n</code></pre>"},{"location":"concepts/llms/#next-steps","title":"Next Steps","text":"<ul> <li>Agents: Use LLMs with agents</li> <li>Tools: Add tools to LLMs</li> <li>Examples: See LLM usage examples</li> </ul>"},{"location":"concepts/llms/#examples","title":"Examples","text":"<p>Check out:</p> <ul> <li><code>examples/llm-usage/main.py</code> - Direct LLM usage</li> <li><code>examples/function-calling/main.py</code> - Function calling</li> <li><code>examples/embeddings/main.py</code> - Embeddings usage</li> </ul>"},{"location":"concepts/memory/","title":"Memory","text":"<p>Memory allows agents to remember conversation history and context across multiple interactions.</p>"},{"location":"concepts/memory/#what-is-memory","title":"What is Memory?","text":"<p>Without memory, each agent call is independent:</p> <pre><code>agent = build_agent('react', llm='openai:gpt-4o-mini', tools=[...])\n\nagent.run(\"My name is Alice\")\n# Agent: \"Nice to meet you, Alice!\"\n\nagent.run(\"What is my name?\")\n# Agent: \"I don't know your name.\"  No memory\n</code></pre> <p>With memory, agents remember context:</p> <pre><code>from tinygent.memory import BufferChatMemory\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    memory=BufferChatMemory()\n)\n\nagent.run(\"My name is Alice\")\n# Agent: \"Nice to meet you, Alice!\"\n\nagent.run(\"What is my name?\")\n# Agent: \"Your name is Alice!\"  Remembers\n</code></pre>"},{"location":"concepts/memory/#memory-types","title":"Memory Types","text":"<p>Tinygent provides 4 built-in memory types:</p>"},{"location":"concepts/memory/#1-bufferchatmemory","title":"1. BufferChatMemory","text":"<p>Best for: Short conversations, full history needed</p> <p>Stores all messages in a list:</p> <pre><code>from tinygent.memory import BufferChatMemory\n\nmemory = BufferChatMemory()\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    memory=memory\n)\n\nagent.run(\"Hello\")\nagent.run(\"My name is Bob\")\nagent.run(\"What's my name?\")\n\n# View history\nprint(memory.load_variables())\n# [\n#   HumanMessage(\"Hello\"),\n#   AIMessage(\"Hi there!\"),\n#   HumanMessage(\"My name is Bob\"),\n#   AIMessage(\"Nice to meet you, Bob!\"),\n#   HumanMessage(\"What's my name?\"),\n# ]\n</code></pre> <p>Pros:</p> <ul> <li>Simple and reliable</li> <li>Complete conversation history</li> <li>No information loss</li> </ul> <p>Cons:</p> <ul> <li>Grows unbounded</li> <li>Can exceed token limits</li> <li>Expensive for long conversations</li> </ul>"},{"location":"concepts/memory/#2-summarybuffermemory","title":"2. SummaryBufferMemory","text":"<p>Best for: Long conversations, summarization acceptable</p> <p>Summarizes old messages to save tokens:</p> <pre><code>from tinygent.memory import SummaryBufferMemory\n\nmemory = SummaryBufferMemory(\n    llm=build_llm('openai:gpt-4o-mini'),\n    max_token_limit=500,  # Summarize when exceeded\n)\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    memory=memory\n)\n\n# After many messages, old ones get summarized\nagent.run(\"Tell me about AI\")  # 200 tokens\nagent.run(\"What about ML?\")    # 200 tokens\nagent.run(\"And deep learning?\") # 200 tokens\n# Now at 600 tokens \u2192 triggers summary\n\n# Old messages condensed to summary\nprint(memory.load_variables())\n# [\n#   SystemMessage(\"Summary: User asked about AI and ML...\"),\n#   HumanMessage(\"And deep learning?\"),\n#   AIMessage(\"Deep learning is...\"),\n# ]\n</code></pre> <p>Pros:</p> <ul> <li>Handles long conversations</li> <li>Prevents token limit issues</li> <li>Maintains key information</li> </ul> <p>Cons:</p> <ul> <li>Loses details in summary</li> <li>Extra LLM calls for summarization</li> <li>May miss nuances</li> </ul>"},{"location":"concepts/memory/#3-windowbuffermemory","title":"3. WindowBufferMemory","text":"<p>Best for: Recent context only, sliding window</p> <p>Keeps only the last N messages:</p> <pre><code>from tinygent.memory import WindowBufferMemory\n\nmemory = WindowBufferMemory(window_size=4)  # Keep last 4 messages\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    memory=memory\n)\n\nagent.run(\"Message 1\")\nagent.run(\"Message 2\")\nagent.run(\"Message 3\")\nagent.run(\"Message 4\")\n\n# Window is full: [User1, AI1, User2, AI2]\n\nagent.run(\"Message 5\")\n\n# Oldest message dropped: [User2, AI2, User3, AI3]\n</code></pre> <p>Pros:</p> <ul> <li>Predictable memory usage</li> <li>Fast and simple</li> <li>Good for recent context</li> </ul> <p>Cons:</p> <ul> <li>Forgets old information</li> <li>No long-term memory</li> <li>May lose important context</li> </ul>"},{"location":"concepts/memory/#4-combinedmemory","title":"4. CombinedMemory","text":"<p>Best for: Multiple memory strategies simultaneously</p> <p>Combine different memory types:</p> <pre><code>from tinygent.memory import CombinedMemory\nfrom tinygent.memory import BufferChatMemory\nfrom tinygent.memory import WindowBufferMemory\n\n# Full history + recent window\ncombined = CombinedMemory(\n    memories={\n        'full_history': BufferChatMemory(),\n        'recent': WindowBufferMemory(window_size=6),\n    }\n)\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    memory=combined\n)\n\n# Both memories updated simultaneously\nagent.run(\"Important information from the start\")\n# ... many messages ...\nagent.run(\"Recent question\")\n\n# Access specific memory\nfull = combined.memories['full_history'].load_variables()\nrecent = combined.memories['recent'].load_variables()\n</code></pre> <p>Pros:</p> <ul> <li>Flexible combinations</li> <li>Multiple access patterns</li> <li>Customizable strategies</li> </ul> <p>Cons:</p> <ul> <li>More complex setup</li> <li>Higher memory usage</li> </ul>"},{"location":"concepts/memory/#memory-operations","title":"Memory Operations","text":""},{"location":"concepts/memory/#saving-context","title":"Saving Context","text":"<p>Manually save messages:</p> <pre><code>from tinygent.core.datamodels.messages import TinyHumanMessage, TinyChatMessage\n\nmemory = BufferChatMemory()\n\n# Save user message\nuser_msg = TinyHumanMessage(content=\"Hello\")\nmemory.save_context(user_msg)\n\n# Save AI response\nai_msg = TinyChatMessage(content=\"Hi there!\")\nmemory.save_context(ai_msg)\n</code></pre>"},{"location":"concepts/memory/#loading-variables","title":"Loading Variables","text":"<p>Retrieve conversation history:</p> <pre><code># Get all messages\nmessages = memory.load_variables()\n\nfor msg in messages:\n    print(f\"{msg.role}: {msg.content}\")\n# human: Hello\n# assistant: Hi there!\n</code></pre>"},{"location":"concepts/memory/#clearing-memory","title":"Clearing Memory","text":"<p>Reset conversation:</p> <pre><code>memory.clear()\n\n# Memory is now empty\nprint(memory.load_variables())  # []\n</code></pre>"},{"location":"concepts/memory/#message-types","title":"Message Types","text":"<p>Tinygent supports multiple message types:</p> <pre><code>from tinygent.core.datamodels.messages import (\n    TinyHumanMessage,      # User messages\n    TinyChatMessage,       # AI responses\n    TinySystemMessage,     # System prompts\n    TinyPlanMessage,       # Planning messages\n    TinyToolMessage,       # Tool results\n)\n\nmemory = BufferChatMemory()\n\nmemory.save_context(TinySystemMessage(content=\"You are a helpful assistant\"))\nmemory.save_context(TinyHumanMessage(content=\"Hello\"))\nmemory.save_context(TinyChatMessage(content=\"Hi there!\"))\nmemory.save_context(TinyPlanMessage(content=\"Plan: 1. Greet user 2. Ask how to help\"))\n</code></pre>"},{"location":"concepts/memory/#memory-filtering","title":"Memory Filtering","text":"<p>Filter messages by type:</p> <pre><code>from tinygent.core.datamodels.messages import TinyHumanMessage, TinyChatMessage\n\nmemory = BufferChatMemory()\n\n# Add various messages\nmemory.save_context(TinyHumanMessage(content=\"User message 1\"))\nmemory.save_context(TinyChatMessage(content=\"AI response 1\"))\nmemory.save_context(TinyHumanMessage(content=\"User message 2\"))\nmemory.save_context(TinyChatMessage(content=\"AI response 2\"))\n\n# Add filter: only human messages\nmemory._chat_history.add_filter(\n    'only_human',\n    lambda m: isinstance(m, TinyHumanMessage)\n)\n\nprint(memory._chat_history)\n# Only shows:\n# - User message 1\n# - User message 2\n\n# Remove filter\nmemory._chat_history.remove_filter('only_human')\n</code></pre>"},{"location":"concepts/memory/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"concepts/memory/#custom-memory","title":"Custom Memory","text":"<p>Create custom memory classes:</p> <pre><code>from tinygent.memory import BaseMemory\n\nclass KeywordMemory(BaseMemory):\n    \"\"\"Memory that only saves messages containing keywords.\"\"\"\n\n    def __init__(self, keywords: list[str]):\n        super().__init__()\n        self.keywords = keywords\n        self.messages = []\n\n    def save_context(self, message):\n        # Only save if contains keyword\n        if any(kw in message.content for kw in self.keywords):\n            self.messages.append(message)\n\n    def load_variables(self):\n        return self.messages\n\n    def clear(self):\n        self.messages = []\n\n# Use it\nmemory = KeywordMemory(keywords=['important', 'urgent', 'critical'])\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    memory=memory\n)\n\nagent.run(\"This is important information\")  # Saved\nagent.run(\"Just casual chat\")                # Not saved\nagent.run(\"Urgent: respond ASAP\")           # Saved\n</code></pre>"},{"location":"concepts/memory/#persistent-memory","title":"Persistent Memory","text":"<p>Save memory to disk:</p> <pre><code>import json\nfrom pathlib import Path\n\ndef save_memory(memory, filepath: str):\n    \"\"\"Save memory to JSON file.\"\"\"\n    messages = [\n        {'role': msg.role, 'content': msg.content}\n        for msg in memory.load_variables()\n    ]\n    Path(filepath).write_text(json.dumps(messages, indent=2))\n\ndef load_memory(filepath: str) -&gt; BufferChatMemory:\n    \"\"\"Load memory from JSON file.\"\"\"\n    memory = BufferChatMemory()\n    messages = json.loads(Path(filepath).read_text())\n\n    for msg in messages:\n        if msg['role'] == 'human':\n            memory.save_context(TinyHumanMessage(content=msg['content']))\n        elif msg['role'] == 'assistant':\n            memory.save_context(TinyChatMessage(content=msg['content']))\n\n    return memory\n\n# Usage\nmemory = BufferChatMemory()\nagent = build_agent('react', llm='openai:gpt-4o-mini', memory=memory)\n\nagent.run(\"Remember this\")\nsave_memory(memory, 'conversation.json')\n\n# Later...\nmemory = load_memory('conversation.json')\nagent = build_agent('react', llm='openai:gpt-4o-mini', memory=memory)\nagent.run(\"What did I say earlier?\")  # Remembers from disk\n</code></pre>"},{"location":"concepts/memory/#memory-with-multistep-agent","title":"Memory with MultiStep Agent","text":"<p>MultiStep agents benefit from memory:</p> <pre><code>from tinygent.agents.multi_step_agent import TinyMultiStepAgent\nfrom tinygent.memory import BufferChatMemory\n\nagent = TinyMultiStepAgent(\n    llm=build_llm('openai:gpt-4o'),\n    tools=[...],\n    memory=BufferChatMemory(),\n)\n\n# First task\nagent.run(\"Plan a trip to Prague\")\n# Agent creates plan, executes steps, remembers results\n\n# Second task - can reference previous context\nagent.run(\"Update the plan based on weather\")\n# Agent remembers previous plan and updates it\n</code></pre>"},{"location":"concepts/memory/#choosing-the-right-memory","title":"Choosing the Right Memory","text":"Use Case Memory Type Why Chatbot (short sessions) BufferChatMemory Full history, simple Long conversations SummaryBufferMemory Prevents token overflow Recent context only WindowBufferMemory Fast, bounded Complex workflows CombinedMemory Multiple strategies Debugging BufferChatMemory Full visibility Production chatbot SummaryBufferMemory Scalable"},{"location":"concepts/memory/#best-practices","title":"Best Practices","text":""},{"location":"concepts/memory/#1-clear-memory-when-needed","title":"1. Clear Memory When Needed","text":"<pre><code># Start fresh conversation\nif user_says_reset:\n    memory.clear()\n</code></pre>"},{"location":"concepts/memory/#2-monitor-memory-size","title":"2. Monitor Memory Size","text":"<pre><code>messages = memory.load_variables()\nif len(messages) &gt; 50:\n    print(\"Warning: Memory getting large\")\n</code></pre>"},{"location":"concepts/memory/#3-use-summaries-for-long-chats","title":"3. Use Summaries for Long Chats","text":"<pre><code># For customer support (long sessions)\nmemory = SummaryBufferMemory(\n    llm=build_llm('openai:gpt-4o-mini'),\n    max_token_limit=1000,\n)\n</code></pre>"},{"location":"concepts/memory/#4-window-for-short-context","title":"4. Window for Short Context","text":"<pre><code># For quick Q&amp;A (no long-term memory needed)\nmemory = WindowBufferMemory(window_size=4)\n</code></pre>"},{"location":"concepts/memory/#memory-and-middleware","title":"Memory and Middleware","text":"<p>Track memory changes with middleware:</p> <pre><code>from tinygent.agents.middleware import TinyBaseMiddleware\n\nclass MemoryMonitorMiddleware(TinyBaseMiddleware):\n    def on_answer(self, *, run_id: str, answer: str) -&gt; None:\n        # Check memory size after each answer\n        size = len(str(agent.memory.load_variables()))\n        print(f\"Memory size: {size} characters\")\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    memory=BufferChatMemory(),\n    middleware=[MemoryMonitorMiddleware()]\n)\n</code></pre>"},{"location":"concepts/memory/#next-steps","title":"Next Steps","text":"<ul> <li>Agents: Use memory with agents</li> <li>Middleware: Monitor memory with middleware</li> <li>Examples: See memory examples</li> </ul>"},{"location":"concepts/memory/#examples","title":"Examples","text":"<p>Check out:</p> <ul> <li><code>examples/memory/basic-chat-memory/main.py</code> - Buffer memory</li> <li><code>examples/memory/buffer-summary-memory/main.py</code> - Summary memory</li> <li><code>examples/memory/buffer-window-chat-memory/main.py</code> - Window memory</li> <li><code>examples/memory/combined-memory/main.py</code> - Combined memory</li> </ul>"},{"location":"concepts/middleware/","title":"Middleware","text":"<p>Middleware allows you to customize agent behavior by hooking into key events during agent execution.</p>"},{"location":"concepts/middleware/#what-is-middleware","title":"What is Middleware?","text":"<p>Middleware provides lifecycle hooks for agents:</p> <ul> <li>Before/after LLM calls: Log prompts, track costs</li> <li>Before/after tool calls: Audit, validate, cache</li> <li>On reasoning: Monitor agent thoughts</li> <li>On errors: Handle failures gracefully</li> <li>On answers: Process final outputs</li> </ul> <p>Think of middleware as event listeners for agent operations.</p>"},{"location":"concepts/middleware/#basic-example","title":"Basic Example","text":"<pre><code>from tinygent.agents.middleware import TinyBaseMiddleware\n\nclass LoggingMiddleware(TinyBaseMiddleware):\n    def on_reasoning(self, *, run_id: str, reasoning: str, **kwargs) -&gt; None:\n        print(f\"Thought: {reasoning}\")\n\n    def before_tool_call(self, *, run_id: str, tool, args, **kwargs) -&gt; None:\n        print(f\"Calling: {tool.info.name}({args})\")\n\n    def after_tool_call(self, *, run_id: str, tool, args, result, **kwargs) -&gt; None:\n        print(f\"Result: {result}\")\n\n    def on_answer(self, *, run_id: str, answer: str, **kwargs) -&gt; None:\n        print(f\"Final Answer: {answer}\")\n\n# Use it\nfrom tinygent.core.factory import build_agent\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n    middleware=[LoggingMiddleware()]\n)\n\nresult = agent.run('What is the weather in Prague?')\n</code></pre> <p>Output:</p> <pre><code>Thought: I need to check the weather in Prague\nCalling: get_weather({'location': 'Prague'})\nResult: The weather in Prague is sunny with a high of 75\u00b0F\nThought: I have the information needed\nFinal Answer: The weather in Prague is sunny with a high of 75\u00b0F.\n</code></pre>"},{"location":"concepts/middleware/#middleware-hooks","title":"Middleware Hooks","text":""},{"location":"concepts/middleware/#agent-lifecycle","title":"Agent Lifecycle","text":"<pre><code>class TinyBaseMiddleware:\n    def on_start(self, *, run_id: str, task: str, **kwargs) -&gt; None:\n        \"\"\"Called when agent starts processing a task.\"\"\"\n        pass\n\n    def on_end(self, *, run_id: str, **kwargs) -&gt; None:\n        \"\"\"Called when agent finishes processing.\"\"\"\n        pass\n\n    def on_error(self, *, run_id: str, e: Exception, **kwargs) -&gt; None:\n        \"\"\"Called when an error occurs.\"\"\"\n        pass\n</code></pre>"},{"location":"concepts/middleware/#llm-calls","title":"LLM Calls","text":"<pre><code>class TinyBaseMiddleware:\n    def before_llm_call(self, *, run_id: str, llm_input, **kwargs) -&gt; None:\n        \"\"\"Called before making an LLM API call.\"\"\"\n        pass\n\n    def after_llm_call(self, *, run_id: str, llm_input, result, **kwargs) -&gt; None:\n        \"\"\"Called after LLM API call completes.\"\"\"\n        pass\n</code></pre>"},{"location":"concepts/middleware/#tool-calls","title":"Tool Calls","text":"<pre><code>class TinyBaseMiddleware:\n    def before_tool_call(self, *, run_id: str, tool, args: dict, **kwargs) -&gt; None:\n        \"\"\"Called before executing a tool.\"\"\"\n        pass\n\n    def after_tool_call(self, *, run_id: str, tool, args: dict, result, **kwargs) -&gt; None:\n        \"\"\"Called after tool execution completes.\"\"\"\n        pass\n</code></pre>"},{"location":"concepts/middleware/#reasoning-and-answers","title":"Reasoning and Answers","text":"<pre><code>class TinyBaseMiddleware:\n    def on_reasoning(self, *, run_id: str, reasoning: str, **kwargs) -&gt; None:\n        \"\"\"Called when agent produces a thought/reasoning step.\"\"\"\n        pass\n\n    def on_answer(self, *, run_id: str, answer: str, **kwargs) -&gt; None:\n        \"\"\"Called when agent produces final answer.\"\"\"\n        pass\n\n    def on_answer_chunk(self, *, run_id: str, chunk: str, idx: str, **kwargs) -&gt; None:\n        \"\"\"Called for each streaming chunk of the answer.\"\"\"\n        pass\n</code></pre>"},{"location":"concepts/middleware/#complete-example-react-cycle-tracker","title":"Complete Example: ReAct Cycle Tracker","text":"<p>Track the Thought-Action-Observation cycle:</p> <pre><code>from typing import Any\nfrom tinygent.agents.middleware import TinyBaseMiddleware\n\nclass ReActCycleMiddleware(TinyBaseMiddleware):\n    def __init__(self) -&gt; None:\n        self.cycles: list[dict[str, Any]] = []\n        self.current_cycle: dict[str, Any] = {}\n        self.iteration = 0\n\n    def on_reasoning(self, *, run_id: str, reasoning: str, **kwargs) -&gt; None:\n        self.iteration += 1\n        self.current_cycle = {\n            'iteration': self.iteration,\n            'thought': reasoning,\n        }\n        print(f\"[Iteration {self.iteration}] {reasoning}\")\n\n    def before_tool_call(self, *, run_id: str, tool, args, **kwargs) -&gt; None:\n        self.current_cycle['action'] = {\n            'tool': tool.info.name,\n            'args': args,\n        }\n        print(f\"[Iteration {self.iteration}] {tool.info.name}({args})\")\n\n    def after_tool_call(self, *, run_id: str, tool, args, result, **kwargs) -&gt; None:\n        self.current_cycle['observation'] = str(result)\n        self.cycles.append(self.current_cycle.copy())\n        print(f\"[Iteration {self.iteration}] {result}\")\n\n    def on_answer(self, *, run_id: str, answer: str, **kwargs) -&gt; None:\n        print(f\"Final Answer after {self.iteration} iterations\")\n\n    def get_summary(self) -&gt; dict[str, Any]:\n        \"\"\"Get execution summary.\"\"\"\n        tools_used = [\n            c.get('action', {}).get('tool')\n            for c in self.cycles\n            if 'action' in c\n        ]\n        return {\n            'total_iterations': self.iteration,\n            'completed_cycles': len(self.cycles),\n            'tools_used': list(set(tools_used)),\n        }\n\n# Usage\nmiddleware = ReActCycleMiddleware()\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    middleware=[middleware]\n)\n\nresult = agent.run('Complex task')\n\n# Get insights\nprint(middleware.get_summary())\n# {\n#   'total_iterations': 3,\n#   'completed_cycles': 3,\n#   'tools_used': ['get_weather', 'search_web']\n# }\n</code></pre>"},{"location":"concepts/middleware/#use-cases","title":"Use Cases","text":""},{"location":"concepts/middleware/#1-performance-monitoring","title":"1. Performance Monitoring","text":"<p>Track LLM call timing:</p> <pre><code>import time\n\nclass TimingMiddleware(TinyBaseMiddleware):\n    def __init__(self) -&gt; None:\n        self.call_start_times: dict[str, float] = {}\n        self.call_durations: list[float] = []\n\n    def before_llm_call(self, *, run_id: str, llm_input, **kwargs) -&gt; None:\n        self.call_start_times[run_id] = time.time()\n\n    def after_llm_call(self, *, run_id: str, llm_input, result, **kwargs) -&gt; None:\n        start = self.call_start_times.pop(run_id, None)\n        if start:\n            duration = time.time() - start\n            self.call_durations.append(duration)\n            print(f\"LLM call took {duration:.2f}s\")\n\n    def get_stats(self) -&gt; dict:\n        if not self.call_durations:\n            return {'avg': 0, 'total': 0}\n\n        return {\n            'total_calls': len(self.call_durations),\n            'avg_duration': sum(self.call_durations) / len(self.call_durations),\n            'total_duration': sum(self.call_durations),\n            'min': min(self.call_durations),\n            'max': max(self.call_durations),\n        }\n</code></pre>"},{"location":"concepts/middleware/#2-tool-auditing","title":"2. Tool Auditing","text":"<p>Log all tool calls for compliance:</p> <pre><code>import json\nfrom datetime import datetime\n\nclass ToolAuditMiddleware(TinyBaseMiddleware):\n    def __init__(self, log_file: str = 'tool_audit.jsonl'):\n        self.log_file = log_file\n\n    def after_tool_call(self, *, run_id: str, tool, args, result, **kwargs) -&gt; None:\n        audit_entry = {\n            'timestamp': datetime.now().isoformat(),\n            'run_id': run_id,\n            'tool': tool.info.name,\n            'args': args,\n            'result': str(result)[:200],  # Truncate\n        }\n\n        # Append to JSONL file\n        with open(self.log_file, 'a') as f:\n            f.write(json.dumps(audit_entry) + '\\n')\n</code></pre>"},{"location":"concepts/middleware/#3-cost-tracking","title":"3. Cost Tracking","text":"<p>Track API costs:</p> <pre><code>class CostTrackingMiddleware(TinyBaseMiddleware):\n    def __init__(self):\n        self.total_cost = 0.0\n        self.costs_by_model = {}\n\n        # Pricing per 1M tokens (example rates)\n        self.pricing = {\n            'gpt-4o-mini': {'input': 0.15, 'output': 0.60},\n            'gpt-4o': {'input': 2.50, 'output': 10.00},\n        }\n\n    def after_llm_call(self, *, run_id: str, llm_input, result, **kwargs) -&gt; None:\n        model = result.model  # e.g., 'gpt-4o-mini'\n        input_tokens = result.usage.prompt_tokens\n        output_tokens = result.usage.completion_tokens\n\n        if model in self.pricing:\n            rates = self.pricing[model]\n            cost = (\n                (input_tokens / 1_000_000) * rates['input'] +\n                (output_tokens / 1_000_000) * rates['output']\n            )\n\n            self.total_cost += cost\n            self.costs_by_model[model] = self.costs_by_model.get(model, 0) + cost\n\n            print(f\"Cost for this call: ${cost:.6f}\")\n\n    def get_total_cost(self) -&gt; float:\n        return self.total_cost\n</code></pre>"},{"location":"concepts/middleware/#4-error-handling","title":"4. Error Handling","text":"<p>Gracefully handle errors:</p> <pre><code>class ErrorHandlingMiddleware(TinyBaseMiddleware):\n    def __init__(self):\n        self.errors: list[dict] = []\n\n    def on_error(self, *, run_id: str, e: Exception, **kwargs) -&gt; None:\n        error_info = {\n            'run_id': run_id,\n            'error_type': type(e).__name__,\n            'message': str(e),\n            'timestamp': datetime.now().isoformat(),\n        }\n\n        self.errors.append(error_info)\n\n        # Log to file\n        with open('errors.log', 'a') as f:\n            f.write(f\"[{error_info['timestamp']}] {error_info['error_type']}: {error_info['message']}\\n\")\n\n        # Send alert (Slack, email, etc.)\n        # self.send_alert(error_info)\n</code></pre>"},{"location":"concepts/middleware/#5-streaming-display","title":"5. Streaming Display","text":"<p>Pretty-print streaming output:</p> <pre><code>class StreamingDisplayMiddleware(TinyBaseMiddleware):\n    def on_answer_chunk(self, *, run_id: str, chunk: str, idx: str, **kwargs) -&gt; None:\n        # Print chunks as they arrive\n        print(chunk, end='', flush=True)\n\n    def on_answer(self, *, run_id: str, answer: str, **kwargs) -&gt; None:\n        # Print newline after complete answer\n        print(\"\\n\")\n</code></pre>"},{"location":"concepts/middleware/#registering-middleware","title":"Registering Middleware","text":""},{"location":"concepts/middleware/#local-registration","title":"Local Registration","text":"<pre><code># Use directly in agent\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    middleware=[LoggingMiddleware(), TimingMiddleware()]\n)\n</code></pre>"},{"location":"concepts/middleware/#global-registration","title":"Global Registration","text":"<p>Make middleware reusable:</p> <pre><code>from tinygent.agents.middleware import TinyBaseMiddleware\n\n@register_middleware('logging')\nclass LoggingMiddleware(TinyBaseMiddleware):\n    # ... implementation ...\n\n# Later, build from registry\nfrom tinygent.core.factory import build_middleware\n\nmiddleware = build_middleware('logging')\nagent = build_agent('react', llm='...', middleware=[middleware])\n</code></pre>"},{"location":"concepts/middleware/#multiple-middleware","title":"Multiple Middleware","text":"<p>Chain multiple middleware together:</p> <pre><code>timing = TimingMiddleware()\nlogging = LoggingMiddleware()\ncost_tracker = CostTrackingMiddleware()\nerror_handler = ErrorHandlingMiddleware()\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    middleware=[timing, logging, cost_tracker, error_handler]\n)\n\nresult = agent.run('Complex task')\n\n# Get insights from each\nprint(f\"Stats: {timing.get_stats()}\")\nprint(f\"Cost: ${cost_tracker.get_total_cost():.4f}\")\nprint(f\"Errors: {len(error_handler.errors)}\")\n</code></pre>"},{"location":"concepts/middleware/#advanced-state-management","title":"Advanced: State Management","text":"<p>Middleware can maintain state across calls:</p> <pre><code>class ConversationMetricsMiddleware(TinyBaseMiddleware):\n    def __init__(self):\n        self.metrics = {\n            'total_turns': 0,\n            'total_tool_calls': 0,\n            'total_tokens': 0,\n            'avg_response_time': 0,\n        }\n        self.start_times = {}\n\n    def on_start(self, *, run_id: str, task: str, **kwargs) -&gt; None:\n        self.start_times[run_id] = time.time()\n        self.metrics['total_turns'] += 1\n\n    def after_tool_call(self, *, run_id: str, tool, args, result, **kwargs) -&gt; None:\n        self.metrics['total_tool_calls'] += 1\n\n    def after_llm_call(self, *, run_id: str, llm_input, result, **kwargs) -&gt; None:\n        self.metrics['total_tokens'] += result.usage.total_tokens\n\n    def on_end(self, *, run_id: str, **kwargs) -&gt; None:\n        start = self.start_times.pop(run_id, None)\n        if start:\n            duration = time.time() - start\n            # Update rolling average\n            prev_avg = self.metrics['avg_response_time']\n            n = self.metrics['total_turns']\n            self.metrics['avg_response_time'] = (prev_avg * (n-1) + duration) / n\n\n    def get_metrics(self) -&gt; dict:\n        return self.metrics\n</code></pre>"},{"location":"concepts/middleware/#best-practices","title":"Best Practices","text":""},{"location":"concepts/middleware/#1-keep-middleware-focused","title":"1. Keep Middleware Focused","text":"<pre><code># Bad - Does too much\nclass GodMiddleware(TinyBaseMiddleware):\n    def on_reasoning(self, *, run_id, reasoning, **kwargs):\n        self.log()\n        self.track_cost()\n        self.send_analytics()\n        self.update_ui()\n\n# Good - Single responsibility\nclass LoggingMiddleware(TinyBaseMiddleware):\n    def on_reasoning(self, *, run_id, reasoning, **kwargs):\n        self.log()\n\nclass CostMiddleware(TinyBaseMiddleware):\n    def on_reasoning(self, *, run_id, reasoning, **kwargs):\n        self.track_cost()\n</code></pre>"},{"location":"concepts/middleware/#2-handle-errors-gracefully","title":"2. Handle Errors Gracefully","text":"<pre><code>class SafeMiddleware(TinyBaseMiddleware):\n    def after_tool_call(self, *, run_id: str, tool, args, result, **kwargs) -&gt; None:\n        try:\n            # Your logic\n            self.process(result)\n        except Exception as e:\n            # Don't crash the agent\n            print(f\"Middleware error: {e}\")\n</code></pre>"},{"location":"concepts/middleware/#3-avoid-blocking-operations","title":"3. Avoid Blocking Operations","text":"<pre><code># Bad - Blocks agent execution\nclass SlowMiddleware(TinyBaseMiddleware):\n    def before_llm_call(self, *, run_id, llm_input, **kwargs):\n        time.sleep(5)  # Blocks!\n\n# Good - Async for I/O\nclass AsyncMiddleware(TinyBaseMiddleware):\n    async def before_llm_call(self, *, run_id, llm_input, **kwargs):\n        await async_operation()\n</code></pre>"},{"location":"concepts/middleware/#middleware-vs-tools","title":"Middleware vs. Tools","text":"<p>Use middleware for:</p> <ul> <li>Logging and monitoring</li> <li>Cost tracking</li> <li>Performance metrics</li> <li>Error handling</li> <li>Auditing</li> </ul> <p>Use tools for:</p> <ul> <li>External API calls</li> <li>Data retrieval</li> <li>Computations</li> <li>Actions (sending emails, etc.)</li> </ul>"},{"location":"concepts/middleware/#agent-specific-hook-activation","title":"Agent-Specific Hook Activation","text":"<p>Different agent types activate different hooks based on their implementation:</p>"},{"location":"concepts/middleware/#tinymultistepagent","title":"TinyMultiStepAgent","text":"<p>Activates: - <code>before_llm_call</code> / <code>after_llm_call</code> - For LLM calls - <code>before_tool_call</code> / <code>after_tool_call</code> - For tool executions - <code>on_plan</code> - When creating initial or updated plan - <code>on_reasoning</code> - For agent reasoning steps - <code>on_tool_reasoning</code> - When reasoning tools generate reasoning - <code>on_answer</code> / <code>on_answer_chunk</code> - For final answers - <code>on_error</code> - On any error</p>"},{"location":"concepts/middleware/#tinyreactagent","title":"TinyReactAgent","text":"<p>Activates: - <code>before_llm_call</code> / <code>after_llm_call</code> - For LLM calls - <code>before_tool_call</code> / <code>after_tool_call</code> - For tool executions - <code>on_tool_reasoning</code> - When reasoning tools generate reasoning - <code>on_answer</code> / <code>on_answer_chunk</code> - For final answers - <code>on_error</code> - On any error</p> <p>Note: React agent does not use <code>on_plan</code> or <code>on_reasoning</code> hooks.</p>"},{"location":"concepts/middleware/#tinymapagent","title":"TinyMAPAgent","text":"<p>Activates: - <code>before_llm_call</code> / <code>after_llm_call</code> - For LLM calls - <code>before_tool_call</code> / <code>after_tool_call</code> - For tool executions - <code>on_plan</code> - When creating search/action plans - <code>on_answer</code> / <code>on_answer_chunk</code> - For final answers - <code>on_error</code> - On any error</p> <p>Note: MAP agent uses <code>on_plan</code> for action summaries but not <code>on_reasoning</code> or <code>on_tool_reasoning</code>.</p>"},{"location":"concepts/middleware/#tinysquadagent","title":"TinySquadAgent","text":"<p>Activates: - <code>before_llm_call</code> / <code>after_llm_call</code> - For LLM calls (delegated to sub-agents) - <code>before_tool_call</code> / <code>after_tool_call</code> - For tool executions (delegated to sub-agents) - <code>on_answer</code> / <code>on_answer_chunk</code> - For final aggregated answers - <code>on_error</code> - On any error</p> <p>Note: Squad agent delegates most hooks to its sub-agents. Hook activation depends on sub-agent types.</p>"},{"location":"concepts/middleware/#built-in-middleware","title":"Built-in Middleware","text":"<p>Tinygent provides ready-to-use middleware for common use cases.</p>"},{"location":"concepts/middleware/#tinytoolcalllimitermiddleware","title":"TinyToolCallLimiterMiddleware","text":"<p>Limits the number of tool calls per agent run. Can operate in two modes: - Global limiter: Limits all tool calls when <code>tool_name=None</code> - Single tool limiter: Limits specific tool by name when <code>tool_name</code> is set</p> <p>When the limit is reached, the behavior depends on <code>hard_block</code>: - hard_block=True: Blocks tool execution and returns error result - hard_block=False: Allows execution but adds system message asking LLM to stop</p> <p>Features: - Limit all tools globally or specific tools individually - Hard block or soft limit behavior - Per-run tracking with automatic cleanup - Statistics tracking</p> <p>Basic Usage:</p> <pre><code>from tinygent.agents.middleware import TinyToolCallLimiterMiddleware\nfrom tinygent.agents import TinyMultiStepAgent\nfrom tinygent.core.factory import build_llm\n\n# Limit all tools to 5 calls\nlimiter = TinyToolCallLimiterMiddleware(max_tool_calls=5)\n\nagent = TinyMultiStepAgent(\n    llm=build_llm('openai:gpt-4o-mini'),\n    tools=[search, calculator, database],\n    middleware=[limiter],\n)\n</code></pre> <p>Limit Specific Tool:</p> <pre><code># Only limit expensive API calls\napi_limiter = TinyToolCallLimiterMiddleware(\n    tool_name='web_search',\n    max_tool_calls=3\n)\n</code></pre> <p>Hard Block vs Soft Limit:</p> <pre><code># Hard block: returns error result when limit reached (default)\nhard_limiter = TinyToolCallLimiterMiddleware(\n    max_tool_calls=5,\n    hard_block=True\n)\n\n# Soft limit: adds system message asking LLM to stop but allows execution\nsoft_limiter = TinyToolCallLimiterMiddleware(\n    max_tool_calls=5,\n    hard_block=False\n)\n</code></pre> <p>Multiple Limiters:</p> <pre><code>middleware = [\n    TinyToolCallLimiterMiddleware(tool_name='web_search', max_tool_calls=3),\n    TinyToolCallLimiterMiddleware(tool_name='database_query', max_tool_calls=10),\n]\n</code></pre> <p>Using Config Factory:</p> <pre><code>from tinygent.agents.middleware import TinyToolCallLimiterMiddlewareConfig\n\n# Create via config\nconfig = TinyToolCallLimiterMiddlewareConfig(\n    tool_name='web_search',\n    max_tool_calls=5,\n    hard_block=True\n)\n\nlimiter = config.build()\n</code></pre> <p>Factory Configuration Options:</p> Field Type Default Description <code>type</code> <code>Literal['tool_limiter']</code> <code>'tool_limiter'</code> Type identifier (frozen) <code>tool_name</code> <code>str \\| None</code> <code>None</code> Specific tool to limit. <code>None</code> = limit all tools globally <code>max_tool_calls</code> <code>int</code> <code>10</code> Maximum number of tool calls allowed per run <code>hard_block</code> <code>bool</code> <code>True</code> Whether to hard block (<code>True</code>) or soft limit (<code>False</code>) tool calls <p>Getting Statistics:</p> <pre><code>stats = limiter.get_stats()\n# {\n#     'tool_name': 'web_search',\n#     'max_tool_calls': 3,\n#     'hard_block': True,\n#     'active_runs': 0,\n#     'current_counts': {},\n#     'runs_at_limit': 0\n# }\n</code></pre>"},{"location":"concepts/middleware/#tinyllmtoolselectormiddleware","title":"TinyLLMToolSelectorMiddleware","text":"<p>Intelligently selects the most relevant subset of tools for each LLM call using a smaller LLM. This middleware is especially useful when you have many tools available but want to reduce context size and improve performance by only providing the most relevant tools to the main agent.</p> <p>Features: - Uses a dedicated LLM to select relevant tools based on conversation context - Reduces token usage by limiting tools sent to the main LLM - Supports always-include list for critical tools - Configurable maximum tools limit - Automatic prompt template management</p> <p>How It Works: 1. Before each LLM call, the middleware analyzes the conversation context 2. Uses a selection LLM to determine which tools are most relevant 3. Filters the tool list to only include selected tools 4. The main agent LLM receives only the relevant subset</p> <p>Basic Usage:</p> <pre><code>from tinygent.agents.middleware import TinyLLMToolSelectorMiddleware\nfrom tinygent.agents import TinyMultiStepAgent\nfrom tinygent.core.factory import build_llm\n\n# Use fast model for tool selection\nselector = TinyLLMToolSelectorMiddleware(\n    llm=build_llm('openai:gpt-4o-mini'),\n    max_tools=5\n)\n\nagent = TinyMultiStepAgent(\n    llm=build_llm('openai:gpt-4o'),\n    tools=[search, calculator, weather, database, email, calendar, notes],\n    middleware=[selector],\n)\n</code></pre> <p>Always Include Critical Tools:</p> <pre><code># Ensure certain tools are always available\nselector = TinyLLMToolSelectorMiddleware(\n    llm=build_llm('openai:gpt-4o-mini'),\n    max_tools=5,\n    always_include=['search', 'calculator']\n)\n</code></pre> <p>Custom Prompt Template:</p> <pre><code>from tinygent.prompts.middleware import LLMToolSelectorPromptTemplate\n\ncustom_prompt = LLMToolSelectorPromptTemplate(\n    system=\"You are a tool selection expert. Select only the most relevant tools.\",\n    user=\"Available tools:\\n{{ tools }}\\n\\nSelect the best tools for the current task.\"\n)\n\nselector = TinyLLMToolSelectorMiddleware(\n    llm=build_llm('openai:gpt-4o-mini'),\n    prompt_template=custom_prompt,\n    max_tools=3\n)\n</code></pre> <p>Using Config Factory:</p> <pre><code>from tinygent.agents.middleware import TinyLLMToolSelectorMiddlewareConfig\n\n# Create via config\nconfig = TinyLLMToolSelectorMiddlewareConfig(\n    llm='openai:gpt-4o-mini',\n    max_tools=5,\n    always_include=['search', 'calculator']\n)\n\nselector = config.build()\n</code></pre> <p>Factory Configuration Options:</p> Field Type Default Description <code>type</code> <code>Literal['llm_tool_selector']</code> <code>'llm_tool_selector'</code> Type identifier (frozen) <code>llm</code> <code>AbstractLLMConfig \\| AbstractLLM</code> Required LLM to use for tool selection. Can be a string like <code>'openai:gpt-4o-mini'</code> or an LLM instance <code>prompt_template</code> <code>LLMToolSelectorPromptTemplate</code> Default prompt Template for tool selection prompt. Contains <code>system</code> and <code>user</code> fields <code>max_tools</code> <code>int \\| None</code> <code>None</code> Maximum number of tools to select. <code>None</code> = no limit <code>always_include</code> <code>list[str] \\| None</code> <code>None</code> List of tool names to always include in selection <p>Advanced Example:</p> <pre><code># Combine with tool limiter\nselector = TinyLLMToolSelectorMiddleware(\n    llm=build_llm('openai:gpt-4o-mini'),\n    max_tools=5,\n    always_include=['search']\n)\n\nlimiter = TinyToolCallLimiterMiddleware(\n    max_tool_calls=10,\n    hard_block=False\n)\n\nagent = TinyMultiStepAgent(\n    llm=build_llm('openai:gpt-4o'),\n    tools=[search, calculator, weather, database, email, calendar, notes, api_call],\n    middleware=[selector, limiter],\n)\n</code></pre> <p>Benefits: - Reduced Token Usage: Only send relevant tools to the main LLM - Improved Performance: Faster LLM responses with smaller context - Better Focus: Agent focuses on appropriate tools for the task - Cost Savings: Fewer tokens = lower API costs</p> <p>When to Use: - You have 10+ tools available - Tools have large descriptions - You want to optimize token usage - You need dynamic tool selection based on context</p>"},{"location":"concepts/middleware/#next-steps","title":"Next Steps","text":"<ul> <li>Agents: Use middleware with agents</li> <li>Examples: See middleware examples</li> <li>Building Agents Guide: Build custom agents with middleware</li> </ul>"},{"location":"concepts/middleware/#examples","title":"Examples","text":"<p>Check out:</p> <ul> <li><code>examples/agents/middleware/main.py</code> - Multiple middleware examples</li> <li><code>examples/agents/middleware/tool_limiter_example.py</code> - Tool call limiting examples</li> <li><code>examples/agents/react/main.py</code> - ReAct cycle tracking</li> <li><code>examples/tracing/main.py</code> - Advanced tracing middleware</li> </ul>"},{"location":"concepts/tools/","title":"Tools","text":"<p>Tools are functions that agents can call to interact with the world. Tinygent makes it easy to turn any Python function into an agent-compatible tool.</p>"},{"location":"concepts/tools/#what-is-a-tool","title":"What is a Tool?","text":"<p>A tool is a Python function that:</p> <ol> <li>Has a clear purpose (described in docstring)</li> <li>Has typed parameters (for schema generation)</li> <li>Returns a value (for agent observation)</li> <li>Is decorated with <code>@tool</code>, <code>@register_tool</code>, <code>@reasoning_tool</code>, or <code>@jit_tool</code></li> </ol> <p>Example:</p> <pre><code>from tinygent.tools import tool\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The city or location to get weather for\n\n    Returns:\n        A string describing the weather\n    \"\"\"\n    return f'The weather in {location} is sunny with a high of 75\u00b0F.'\n</code></pre> <p>When you pass this tool to an agent, the LLM can:</p> <ul> <li>See the function name: <code>get_weather</code></li> <li>See the description: <code>\"Get the current weather...\"</code></li> <li>See the parameters: <code>location: str</code></li> <li>Call it when needed: <code>get_weather(location=\"Prague\")</code></li> <li>Use the result: <code>\"The weather in Prague is sunny...\"</code></li> </ul>"},{"location":"concepts/tools/#tool-decorators","title":"Tool Decorators","text":"<p>Tinygent provides 4 tool decorators for different use cases:</p>"},{"location":"concepts/tools/#1-tool-simple-tools","title":"1. <code>@tool</code> - Simple Tools","text":"<p>Best for: Quick local tools, no global registration needed</p> <pre><code>from tinygent.tools import tool\n\n@tool\ndef calculator(expression: str) -&gt; float:\n    \"\"\"Evaluate a mathematical expression.\n\n    Args:\n        expression: A valid Python math expression\n\n    Returns:\n        The result of the calculation\n    \"\"\"\n    return eval(expression)\n\n# Use directly in agent\nagent = build_agent('react', llm='openai:gpt-4o-mini', tools=[calculator])\n</code></pre> <p>Features:</p> <ul> <li>Lightweight, no registration</li> <li>Great for simple use cases</li> <li>Perfect for inline definitions</li> </ul>"},{"location":"concepts/tools/#2-register_tool-globally-registered-tools","title":"2. <code>@register_tool</code> - Globally Registered Tools","text":"<p>Best for: Reusable tools, CLI usage, multi-agent systems</p> <pre><code>from tinygent.tools import register_tool\n\n@register_tool(use_cache=True)\ndef search_web(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    # Call web search API\n    return f\"Results for: {query}\"\n\n# Discover from global registry\nfrom tinygent.core.runtime.tool_catalog import GlobalToolCatalog\n\nregistry = GlobalToolCatalog().get_active_catalog()\nsearch = registry.get_tool('search_web')\n</code></pre> <p>Features:</p> <ul> <li>Global discovery: Access from anywhere</li> <li>Caching: Optional result caching with <code>use_cache=True</code></li> <li>CLI support: Usable in <code>tiny</code> CLI terminal</li> <li>Reusability: Share across multiple agents</li> </ul> <p>Caching Example:</p> <pre><code>@register_tool(use_cache=True)\ndef expensive_api_call(query: str) -&gt; str:\n    \"\"\"Call an expensive API.\"\"\"\n    import time\n    time.sleep(2)  # Simulate slow API\n    return f\"Result for {query}\"\n\n# First call: Takes 2 seconds\nresult1 = expensive_api_call(query=\"test\")\n\n# Second call with same args: Instant (cached)\nresult2 = expensive_api_call(query=\"test\")\n\n# Check cache stats\nprint(expensive_api_call.cache_info())\n# CacheInfo(hits=1, misses=1, maxsize=128, currsize=1)\n\n# Clear cache\nexpensive_api_call.clear_cache()\n</code></pre>"},{"location":"concepts/tools/#3-reasoning_tool-tools-with-reasoning","title":"3. <code>@reasoning_tool</code> - Tools with Reasoning","text":"<p>Best for: Complex operations that benefit from explaining \"why\"</p> <p>Reasoning tools require the agent to provide a rationale before calling:</p> <pre><code>from tinygent.tools import register_reasoning_tool\n\n@register_reasoning_tool(\n    reasoning_prompt='Explain why you are performing this search.'\n)\ndef search_database(query: str) -&gt; str:\n    \"\"\"Search the internal database.\"\"\"\n    return f\"Database results for: {query}\"\n</code></pre> <p>Agent interaction:</p> <pre><code>Agent: I need to search the database for user information.\nReasoning: I'm searching for \"John Doe\" because the user asked about their account status.\nAction: search_database(query=\"John Doe\")\nObservation: Found user record for John Doe\n</code></pre> <p>When to use:</p> <ul> <li>High-cost operations (API calls, computations)</li> <li>Actions that need justification (delete, modify)</li> <li>Debugging agent decision-making</li> </ul>"},{"location":"concepts/tools/#4-jit_tool-just-in-time-code-generation","title":"4. <code>@jit_tool</code> - Just-In-Time Code Generation","text":"<p>Best for: Dynamic operations, code generation, flexible workflows</p> <p>JIT tools generate and execute code at runtime based on agent instructions:</p> <pre><code>from tinygent.tools import jit_tool\n\n@jit_tool(jit_instruction='Generate code to count from 1 to n, yielding each number.')\ndef count(n: int):\n    \"\"\"Count from 1 to n, yielding each number.\"\"\"\n    for i in range(1, n + 1):\n        yield i\n\n# Agent can dynamically modify behavior\nresult = list(count(n=5))  # [1, 2, 3, 4, 5]\n</code></pre> <p>When to use:</p> <ul> <li>Dynamic code generation</li> <li>Flexible operations</li> <li>When tool behavior needs runtime customization</li> </ul>"},{"location":"concepts/tools/#tool-schemas","title":"Tool Schemas","text":"<p>Tools can accept two input styles:</p>"},{"location":"concepts/tools/#style-1-pydantic-models","title":"Style 1: Pydantic Models","text":"<p>Best for: Complex inputs, validation, documentation</p> <pre><code>from pydantic import Field\nfrom tinygent.core.types import TinyModel\n\nclass WeatherInput(TinyModel):\n    location: str = Field(..., description='The city or location')\n    units: str = Field('celsius', description='Temperature units: celsius or fahrenheit')\n    include_forecast: bool = Field(False, description='Include 5-day forecast')\n\n@register_tool\ndef get_weather(data: WeatherInput) -&gt; str:\n    \"\"\"Get detailed weather information.\"\"\"\n    forecast = ' + 5-day forecast' if data.include_forecast else ''\n    return f\"Weather in {data.location}: 22\u00b0{data.units[0].upper()}{forecast}\"\n</code></pre> <p>Benefits:</p> <ul> <li>Runtime validation (Pydantic)</li> <li>Rich field descriptions</li> <li>Default values</li> <li>Type safety</li> </ul>"},{"location":"concepts/tools/#style-2-regular-parameters","title":"Style 2: Regular Parameters","text":"<p>Best for: Simple inputs, quick tools</p> <pre><code>@register_tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers together.\"\"\"\n    return a * b\n\n# Call with kwargs\nresult = multiply(a=3, b=4)  # 12\n\n# Or with dict\nresult = multiply({'a': 5, 'b': 6})  # 30\n</code></pre> <p>Benefits:</p> <ul> <li>Less boilerplate</li> <li>Quick to write</li> <li>Familiar Python syntax</li> </ul>"},{"location":"concepts/tools/#async-tools","title":"Async Tools","text":"<p>Tools can be async for I/O operations:</p> <pre><code>import httpx\n\n@register_tool\nasync def fetch_url(url: str) -&gt; str:\n    \"\"\"Fetch content from a URL.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        return response.text\n\n# Agents automatically handle async tools\nagent = build_agent('react', llm='openai:gpt-4o-mini', tools=[fetch_url])\nresult = agent.run('Fetch https://example.com')\n</code></pre>"},{"location":"concepts/tools/#generator-tools","title":"Generator Tools","text":"<p>Tools can yield results for streaming:</p> <pre><code>@tool\nasync def stream_data(query: str):\n    \"\"\"Stream data from a source.\"\"\"\n    for i in range(5):\n        await asyncio.sleep(0.5)\n        yield f\"Chunk {i}: {query}\"\n\n# Agent receives results as they arrive\nresult = list(stream_data(query=\"test\"))\n# ['Chunk 0: test', 'Chunk 1: test', ...]\n</code></pre>"},{"location":"concepts/tools/#error-handling","title":"Error Handling","text":"<p>Tools should raise descriptive errors:</p> <pre><code>@tool\ndef divide(a: float, b: float) -&gt; float:\n    \"\"\"Divide two numbers.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\n# Agent receives error and tries alternative approach\nagent = build_agent('react', llm='openai:gpt-4o-mini', tools=[divide])\nresult = agent.run('What is 10 divided by 0?')\n# Agent: \"Division by zero is undefined in mathematics.\"\n</code></pre>"},{"location":"concepts/tools/#tool-composition","title":"Tool Composition","text":"<p>Combine tools for complex workflows:</p> <pre><code>@register_tool\ndef search_products(category: str) -&gt; list[str]:\n    \"\"\"Search for products in a category.\"\"\"\n    return ['Product A', 'Product B', 'Product C']\n\n@register_tool\ndef get_product_details(product_name: str) -&gt; dict:\n    \"\"\"Get detailed information about a product.\"\"\"\n    return {\n        'name': product_name,\n        'price': 99.99,\n        'in_stock': True\n    }\n\n# Agent chains tools\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[search_products, get_product_details]\n)\n\nresult = agent.run('Find electronics and tell me about Product A')\n# Agent calls: search_products('electronics') \u2192 get_product_details('Product A')\n</code></pre>"},{"location":"concepts/tools/#best-practices","title":"Best Practices","text":""},{"location":"concepts/tools/#1-clear-docstrings","title":"1. Clear Docstrings","text":"<pre><code># Bad\n@tool\ndef process(data: str) -&gt; str:\n    \"\"\"Process data.\"\"\"  # Too vague\n    return data.upper()\n\n# Good\n@tool\ndef uppercase_text(text: str) -&gt; str:\n    \"\"\"Convert text to uppercase.\n\n    Args:\n        text: The text to convert\n\n    Returns:\n        The text in uppercase\n\n    Example:\n        uppercase_text(\"hello\") -&gt; \"HELLO\"\n    \"\"\"\n    return text.upper()\n</code></pre>"},{"location":"concepts/tools/#2-type-hints","title":"2. Type Hints","text":"<pre><code># Bad\n@tool\ndef add(a, b):  # No types\n    return a + b\n\n# Good\n@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two integers.\"\"\"\n    return a + b\n</code></pre>"},{"location":"concepts/tools/#3-single-responsibility","title":"3. Single Responsibility","text":"<pre><code># Bad - Does too much\n@tool\ndef fetch_and_analyze_and_summarize(url: str) -&gt; str:\n    \"\"\"Fetch URL, analyze content, and summarize.\"\"\"\n    content = fetch(url)\n    analyzed = analyze(content)\n    return summarize(analyzed)\n\n# Good - Split into focused tools\n@tool\ndef fetch_url(url: str) -&gt; str:\n    \"\"\"Fetch content from a URL.\"\"\"\n    return fetch(url)\n\n@tool\ndef analyze_text(text: str) -&gt; dict:\n    \"\"\"Analyze text content.\"\"\"\n    return analyze(text)\n\n@tool\ndef summarize_analysis(analysis: dict) -&gt; str:\n    \"\"\"Create summary from analysis.\"\"\"\n    return summarize(analysis)\n</code></pre>"},{"location":"concepts/tools/#4-validation","title":"4. Validation","text":"<pre><code>from pydantic import Field, field_validator\n\nclass SearchInput(TinyModel):\n    query: str = Field(..., min_length=1, max_length=200)\n    max_results: int = Field(10, ge=1, le=100)\n\n    @field_validator('query')\n    def validate_query(cls, v):\n        if not v.strip():\n            raise ValueError('Query cannot be empty')\n        return v.strip()\n\n@register_tool\ndef search(data: SearchInput) -&gt; str:\n    \"\"\"Search with validation.\"\"\"\n    return f\"Searching for: {data.query}\"\n</code></pre>"},{"location":"concepts/tools/#tool-registry","title":"Tool Registry","text":"<p>Access registered tools globally:</p> <pre><code>from tinygent.core.runtime.tool_catalog import GlobalToolCatalog\n\n# Get active catalog\ncatalog = GlobalToolCatalog().get_active_catalog()\n\n# List all tools\nall_tools = catalog.list_tools()\nprint(all_tools)  # ['get_weather', 'search_web', 'calculator', ...]\n\n# Get specific tool\nweather_tool = catalog.get_tool('get_weather')\n\n# Call it\nresult = weather_tool(location='Prague')\n</code></pre>"},{"location":"concepts/tools/#advanced-hidden-tools","title":"Advanced: Hidden Tools","text":"<p>Mark tools as hidden for internal use:</p> <pre><code>@register_tool(hidden=True)\ndef internal_helper(data: str) -&gt; str:\n    \"\"\"Internal tool, not exposed to agents.\"\"\"\n    return process_internally(data)\n\n# Not visible in default tool lists\ncatalog = GlobalToolCatalog().get_active_catalog()\nvisible_tools = catalog.list_tools()  # Doesn't include 'internal_helper'\n\n# But still accessible if you know the name\nhelper = catalog.get_tool('internal_helper')\n</code></pre>"},{"location":"concepts/tools/#next-steps","title":"Next Steps","text":"<ul> <li>Agents: Use tools with agents</li> <li>Custom Tools Guide: Build advanced tools</li> <li>Examples: See tool usage examples</li> </ul>"},{"location":"concepts/tools/#examples","title":"Examples","text":"<p>Check out:</p> <ul> <li><code>examples/tool-usage/main.py</code> - All tool decorator types</li> <li><code>examples/function-calling/main.py</code> - Function calling patterns</li> <li><code>packages/tiny_brave/</code> - Real-world tool: Brave search</li> </ul>"},{"location":"guides/building-agents/","title":"Building Agents Guide","text":"<p>A comprehensive guide to building agents with Tinygent.</p>"},{"location":"guides/building-agents/#quick-start","title":"Quick Start","text":""},{"location":"guides/building-agents/#1-simple-agent","title":"1. Simple Agent","text":"<p>The fastest way to create an agent:</p> <pre><code>from tinygent.tools import tool\nfrom tinygent.core.factory import build_agent\n\n@tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather in a given location.\"\"\"\n    return f'The weather in {location} is sunny with a high of 75\u00b0F.'\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n)\n\nresult = agent.run('What is the weather in Prague?')\nprint(result)\n</code></pre>"},{"location":"guides/building-agents/#step-by-step-building-a-travel-agent","title":"Step-by-Step: Building a Travel Agent","text":"<p>Let's build a complete travel planning agent.</p>"},{"location":"guides/building-agents/#step-1-define-tools","title":"Step 1: Define Tools","text":"<pre><code>from pydantic import Field\nfrom tinygent.core.types import TinyModel\nfrom tinygent.tools import register_tool\n\nclass WeatherInput(TinyModel):\n    location: str = Field(..., description='City or location name')\n    days: int = Field(1, description='Number of days for forecast')\n\n@register_tool\ndef get_weather(data: WeatherInput) -&gt; str:\n    \"\"\"Get weather forecast for a location.\"\"\"\n    # In real app, call weather API\n    return f\"{data.days}-day forecast for {data.location}: Sunny and warm\"\n\nclass FlightInput(TinyModel):\n    origin: str = Field(..., description='Departure city')\n    destination: str = Field(..., description='Arrival city')\n    date: str = Field(..., description='Travel date (YYYY-MM-DD)')\n\n@register_tool\ndef search_flights(data: FlightInput) -&gt; str:\n    \"\"\"Search for available flights.\"\"\"\n    # In real app, call flight API\n    return f\"Found 5 flights from {data.origin} to {data.destination} on {data.date}\"\n\nclass HotelInput(TinyModel):\n    location: str = Field(..., description='City name')\n    checkin: str = Field(..., description='Check-in date')\n    checkout: str = Field(..., description='Check-out date')\n    guests: int = Field(1, description='Number of guests')\n\n@register_tool\ndef search_hotels(data: HotelInput) -&gt; str:\n    \"\"\"Search for available hotels.\"\"\"\n    # In real app, call hotel API\n    return f\"Found 10 hotels in {data.location} for {data.guests} guest(s)\"\n</code></pre>"},{"location":"guides/building-agents/#step-2-create-agent-with-memory","title":"Step 2: Create Agent with Memory","text":"<pre><code>from tinygent.core.factory import build_agent\nfrom tinygent.memory import BufferChatMemory\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather, search_flights, search_hotels],\n    memory=BufferChatMemory(),\n    max_iterations=10,\n)\n</code></pre>"},{"location":"guides/building-agents/#step-3-add-middleware-for-logging","title":"Step 3: Add Middleware for Logging","text":"<pre><code>from tinygent.agents.middleware import TinyBaseMiddleware\n\nclass TravelAgentMiddleware(TinyBaseMiddleware):\n    def on_reasoning(self, *, run_id: str, reasoning: str) -&gt; None:\n        print(f\"Planning: {reasoning}\")\n\n    def before_tool_call(self, *, run_id: str, tool, args) -&gt; None:\n        print(f\"Searching: {tool.info.name}\")\n\n    def on_answer(self, *, run_id: str, answer: str) -&gt; None:\n        print(f\"Recommendation ready!\")\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather, search_flights, search_hotels],\n    memory=BufferChatMemory(),\n    middleware=[TravelAgentMiddleware()],\n)\n</code></pre>"},{"location":"guides/building-agents/#step-4-use-the-agent","title":"Step 4: Use the Agent","text":"<pre><code># First conversation\nresult = agent.run(\n    'I want to travel from New York to Paris in June. '\n    'Check the weather and find flights and hotels.'\n)\nprint(result)\n\n# Follow-up conversation (uses memory)\nresult = agent.run(\n    'Actually, make it a 5-day trip and I need accommodations for 2 people.'\n)\nprint(result)\n</code></pre>"},{"location":"guides/building-agents/#advanced-multi-step-planning-agent","title":"Advanced: Multi-Step Planning Agent","text":"<p>For complex workflows, use MultiStep agent:</p> <pre><code>from tinygent.agents.multi_step_agent import TinyMultiStepAgent\nfrom tinygent.core.factory import build_llm\nfrom tinygent.prompts.multistep import (\n    MultiStepPromptTemplate,\n    PlanPromptTemplate,\n    ActionPromptTemplate,\n    FallbackAnswerPromptTemplate,\n)\n\n# Custom prompts\nplan_prompt = PlanPromptTemplate(\n    init_plan=(\n        'Create a detailed travel plan for: {{ task }}\\n'\n        'Available tools: {{ tools }}\\n'\n        'Break down into clear steps.'\n    ),\n    update_plan=(\n        'Update the travel plan based on new information.\\n'\n        'Task: {{ task }}\\n'\n        'Completed steps: {{ steps }}\\n'\n        'Remaining: {{ remaining_steps }}'\n    ),\n)\n\naction_prompt = ActionPromptTemplate(\n    system='You are a professional travel planning assistant.',\n    final_answer=(\n        'Provide a comprehensive travel itinerary for: {{ task }}\\n'\n        'Based on steps completed: {{ steps }}\\n'\n        'And tool results: {{ tool_calls }}'\n    ),\n)\n\nfallback_prompt = FallbackAnswerPromptTemplate(\n    fallback_answer=(\n        'Create final travel plan for: {{ task }}\\n'\n        'Using information from: {{ history }}'\n    )\n)\n\nprompt_template = MultiStepPromptTemplate(\n    plan=plan_prompt,\n    acter=action_prompt,\n    fallback=fallback_prompt,\n)\n\n# Create agent\nagent = TinyMultiStepAgent(\n    llm=build_llm('openai:gpt-4o'),\n    tools=[get_weather, search_flights, search_hotels],\n    prompt_template=prompt_template,\n    memory=BufferChatMemory(),\n    max_iterations=15,\n)\n\nresult = agent.run(\n    'Plan a 7-day vacation to Tokyo in April for 2 people. '\n    'Include flights from San Francisco, hotels, and weather forecast.'\n)\n</code></pre>"},{"location":"guides/building-agents/#multi-agent-system-with-squad","title":"Multi-Agent System with Squad","text":"<p>Coordinate specialized agents:</p> <pre><code>from tinygent.agents.squad_agent import TinySquadAgent\n\n# Create specialized agents\nweather_agent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[get_weather],\n)\n\nbooking_agent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[search_flights, search_hotels],\n)\n\n# Create squad coordinator\nsquad = TinySquadAgent(\n    llm=build_llm('openai:gpt-4o'),\n    agents=[weather_agent, booking_agent],\n    max_iterations=5,\n)\n\nresult = squad.run(\n    'I need a complete travel plan: check weather in Paris, '\n    'find flights from London, and book a hotel for 3 nights.'\n)\n</code></pre>"},{"location":"guides/building-agents/#best-practices","title":"Best Practices","text":""},{"location":"guides/building-agents/#1-start-simple-grow-complex","title":"1. Start Simple, Grow Complex","text":"<pre><code># Phase 1: Single tool, single agent\nagent = build_agent('react', llm='openai:gpt-4o-mini', tools=[weather])\n\n# Phase 2: Multiple tools\nagent = build_agent('react', llm='...', tools=[weather, flights, hotels])\n\n# Phase 3: Add memory\nagent = build_agent('react', llm='...', tools=[...], memory=BufferChatMemory())\n\n# Phase 4: Add middleware\nagent = build_agent('react', llm='...', tools=[...], middleware=[logger])\n\n# Phase 5: Multi-step or Squad\nagent = TinyMultiStepAgent(llm='...', tools=[...])\n</code></pre>"},{"location":"guides/building-agents/#2-use-appropriate-agent-type","title":"2. Use Appropriate Agent Type","text":"<pre><code># Simple Q&amp;A \u2192 ReAct\nagent = build_agent('react', ...)\n\n# Complex planning \u2192 MultiStep\nagent = TinyMultiStepAgent(...)\n\n# Specialized tasks \u2192 Squad\nsquad = TinySquadAgent(agents=[specialist1, specialist2])\n\n# Dynamic replanning \u2192 MAP\nagent = TinyMAPAgent(...)\n</code></pre>"},{"location":"guides/building-agents/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<pre><code>try:\n    result = agent.run('User query')\nexcept Exception as e:\n    print(f\"Agent error: {e}\")\n    # Fallback logic\n    result = \"I'm sorry, I encountered an error. Please try again.\"\n</code></pre>"},{"location":"guides/building-agents/#4-test-with-cheap-models-first","title":"4. Test with Cheap Models First","text":"<pre><code># Development\ndev_agent = build_agent('react', llm='openai:gpt-4o-mini', tools=[...])\n\n# Production\nprod_agent = build_agent('react', llm='openai:gpt-4o', tools=[...])\n</code></pre>"},{"location":"guides/building-agents/#5-monitor-and-log","title":"5. Monitor and Log","text":"<pre><code>from tinygent.logging import setup_logger\n\nlogger = setup_logger('debug')\n\nclass LoggingMiddleware(TinyBaseMiddleware):\n    def on_reasoning(self, *, run_id: str, reasoning: str) -&gt; None:\n        logger.info(f'[{run_id}] Reasoning: {reasoning}')\n</code></pre>"},{"location":"guides/building-agents/#production-checklist","title":"Production Checklist","text":"<p>Before deploying to production:</p> <ul> <li> Error handling: Wrap agent calls in try-except</li> <li> Logging: Add comprehensive logging middleware</li> <li> Cost tracking: Monitor LLM API costs</li> <li> Rate limiting: Implement rate limits for APIs</li> <li> Caching: Cache tool results where appropriate</li> <li> Memory management: Clear or summarize long conversations</li> <li> Monitoring: Track agent performance and errors</li> <li> Security: Validate tool inputs, sanitize outputs</li> <li> Testing: Test with edge cases and failure scenarios</li> <li> Documentation: Document agent capabilities and limitations</li> </ul>"},{"location":"guides/building-agents/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/building-agents/#pattern-1-conversational-agent","title":"Pattern 1: Conversational Agent","text":"<pre><code>from tinygent.memory import BufferChatMemory\n\nagent = build_agent(\n    'react',\n    llm='openai:gpt-4o-mini',\n    tools=[...],\n    memory=BufferChatMemory(),\n)\n\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() in ['quit', 'exit']:\n        break\n\n    response = agent.run(user_input)\n    print(f\"Agent: {response}\")\n</code></pre>"},{"location":"guides/building-agents/#pattern-2-streaming-responses","title":"Pattern 2: Streaming Responses","text":"<pre><code>import asyncio\n\nasync def chat_with_streaming():\n    agent = build_agent('react', llm='openai:gpt-4o-mini', tools=[...])\n\n    print(\"Agent: \", end='', flush=True)\n    async for chunk in agent.run_stream(user_input):\n        print(chunk, end='', flush=True)\n    print()  # Newline\n\nasyncio.run(chat_with_streaming())\n</code></pre>"},{"location":"guides/building-agents/#pattern-3-batch-processing","title":"Pattern 3: Batch Processing","text":"<pre><code>tasks = [\n    'What is the weather in Paris?',\n    'Find flights from London to Tokyo',\n    'Search hotels in New York',\n]\n\nagent = build_agent('react', llm='openai:gpt-4o-mini', tools=[...])\n\nresults = []\nfor task in tasks:\n    result = agent.run(task)\n    results.append(result)\n</code></pre>"},{"location":"guides/building-agents/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Tools Guide: Build advanced tools</li> <li>Core Concepts: Deep dive into agents</li> <li>Examples: More examples</li> </ul>"},{"location":"guides/building-agents/#further-reading","title":"Further Reading","text":"<ul> <li>Agent Architecture: See <code>tinygent/agents/</code> for implementation details</li> <li>Registry Pattern: See <code>tinygent/core/runtime/</code> for global registries</li> <li>Prompts: See <code>tinygent/core/prompts/</code> for prompt templates</li> </ul>"},{"location":"guides/custom-tools/","title":"Building Custom Tools","text":"<p>A comprehensive guide to creating powerful custom tools for your agents.</p>"},{"location":"guides/custom-tools/#quick-start","title":"Quick Start","text":"<pre><code>from tinygent.tools import tool\n\n@tool\ndef hello_world(name: str) -&gt; str:\n    \"\"\"Say hello to someone.\"\"\"\n    return f\"Hello, {name}!\"\n\n# Use it\nfrom tinygent.core.factory import build_agent\n\nagent = build_agent('react', llm='openai:gpt-4o-mini', tools=[hello_world])\nresult = agent.run('Say hello to Alice')\n</code></pre>"},{"location":"guides/custom-tools/#tool-anatomy","title":"Tool Anatomy","text":"<p>Every tool needs three things:</p> <ol> <li>Decorator: <code>@tool</code>, <code>@register_tool</code>, <code>@reasoning_tool</code>, or <code>@jit_tool</code></li> <li>Type hints: For automatic schema generation</li> <li>Docstring: Describes what the tool does</li> </ol> <pre><code>from tinygent.tools import tool\n\n@tool  # 1. Decorator\ndef search_database(query: str, limit: int = 10) -&gt; list[dict]:  # 2. Type hints\n    \"\"\"Search the database for records matching the query.  # 3. Docstring\n\n    Args:\n        query: The search term to look for\n        limit: Maximum number of results to return\n\n    Returns:\n        List of matching records\n    \"\"\"\n    # Implementation\n    results = database.search(query, limit=limit)\n    return results\n</code></pre>"},{"location":"guides/custom-tools/#simple-tools","title":"Simple Tools","text":""},{"location":"guides/custom-tools/#basic-function-tool","title":"Basic Function Tool","text":"<pre><code>@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n</code></pre>"},{"location":"guides/custom-tools/#with-default-values","title":"With Default Values","text":"<pre><code>@tool\ndef greet(name: str, greeting: str = \"Hello\") -&gt; str:\n    \"\"\"Greet someone with a custom greeting.\"\"\"\n    return f\"{greeting}, {name}!\"\n\n# Agent can call:\n# greet(name=\"Alice\") \u2192 \"Hello, Alice!\"\n# greet(name=\"Bob\", greeting=\"Hi\") \u2192 \"Hi, Bob!\"\n</code></pre>"},{"location":"guides/custom-tools/#with-optional-parameters","title":"With Optional Parameters","text":"<pre><code>from typing import Optional\n\n@tool\ndef send_email(to: str, subject: str, body: str, cc: Optional[str] = None) -&gt; str:\n    \"\"\"Send an email.\n\n    Args:\n        to: Recipient email address\n        subject: Email subject line\n        body: Email body content\n        cc: Optional CC recipient\n    \"\"\"\n    message = f\"Sending email to {to}\"\n    if cc:\n        message += f\" (CC: {cc})\"\n    return message\n</code></pre>"},{"location":"guides/custom-tools/#pydantic-model-tools","title":"Pydantic Model Tools","text":"<p>For complex validation and documentation:</p> <pre><code>from pydantic import Field, field_validator, EmailStr\nfrom tinygent.core.types import TinyModel\nfrom tinygent.tools import register_tool\n\nclass EmailInput(TinyModel):\n    to: EmailStr = Field(..., description='Recipient email address')\n    subject: str = Field(..., min_length=1, max_length=100)\n    body: str = Field(..., min_length=1, description='Email body content')\n    priority: str = Field('normal', description='Priority: low, normal, high')\n\n    @field_validator('priority')\n    def validate_priority(cls, v):\n        allowed = ['low', 'normal', 'high']\n        if v not in allowed:\n            raise ValueError(f'Priority must be one of: {allowed}')\n        return v\n\n@register_tool\ndef send_email(data: EmailInput) -&gt; str:\n    \"\"\"Send an email with validation.\"\"\"\n    return f\"Sent {data.priority}-priority email to {data.to}\"\n</code></pre>"},{"location":"guides/custom-tools/#async-tools","title":"Async Tools","text":"<p>For I/O-bound operations:</p> <pre><code>import httpx\n\n@register_tool\nasync def fetch_url(url: str) -&gt; str:\n    \"\"\"Fetch content from a URL.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        return response.text\n\n@register_tool\nasync def query_database(query: str) -&gt; list[dict]:\n    \"\"\"Query async database.\"\"\"\n    async with async_db.connect() as conn:\n        results = await conn.fetch(query)\n        return [dict(row) for row in results]\n</code></pre>"},{"location":"guides/custom-tools/#generator-tools","title":"Generator Tools","text":"<p>For streaming results:</p> <pre><code>@tool\nasync def stream_search_results(query: str) -&gt; str:\n    \"\"\"Stream search results one at a time.\"\"\"\n    for i in range(5):\n        await asyncio.sleep(0.5)\n        yield f\"Result {i+1}: {query}\"\n\n# Agent receives:\n# \"Result 1: my query\"\n# \"Result 2: my query\"\n# ...\n</code></pre>"},{"location":"guides/custom-tools/#error-handling","title":"Error Handling","text":""},{"location":"guides/custom-tools/#validation-errors","title":"Validation Errors","text":"<pre><code>@tool\ndef divide(a: float, b: float) -&gt; float:\n    \"\"\"Divide two numbers.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero. Please use a non-zero divisor.\")\n    return a / b\n\n# Agent will see: \"Error: Cannot divide by zero. Please use a non-zero divisor.\"\n</code></pre>"},{"location":"guides/custom-tools/#api-errors","title":"API Errors","text":"<pre><code>import httpx\n\n@tool\nasync def call_api(endpoint: str) -&gt; dict:\n    \"\"\"Call external API.\"\"\"\n    try:\n        async with httpx.AsyncClient() as client:\n            response = await client.get(endpoint, timeout=5.0)\n            response.raise_for_status()\n            return response.json()\n    except httpx.TimeoutException:\n        raise Exception(\"API request timed out after 5 seconds\")\n    except httpx.HTTPError as e:\n        raise Exception(f\"API error: {e}\")\n</code></pre>"},{"location":"guides/custom-tools/#caching-tools","title":"Caching Tools","text":"<p>Speed up repeated calls:</p> <pre><code>@register_tool(use_cache=True)\ndef expensive_computation(input_data: str) -&gt; str:\n    \"\"\"Perform expensive computation (cached).\"\"\"\n    import time\n    time.sleep(2)  # Simulate slow operation\n    return f\"Computed: {input_data.upper()}\"\n\n# First call: Takes 2 seconds\nresult = expensive_computation(input_data=\"test\")\n\n# Second call with same input: Instant (from cache)\nresult = expensive_computation(input_data=\"test\")\n\n# Different input: Takes 2 seconds again\nresult = expensive_computation(input_data=\"other\")\n\n# Cache stats\nprint(expensive_computation.cache_info())\n# CacheInfo(hits=1, misses=2, maxsize=128, currsize=2)\n\n# Clear cache\nexpensive_computation.clear_cache()\n</code></pre>"},{"location":"guides/custom-tools/#reasoning-tools","title":"Reasoning Tools","text":"<p>Require the agent to explain its reasoning:</p> <pre><code>from tinygent.tools import register_reasoning_tool\n\n@register_reasoning_tool(\n    reasoning_prompt='Explain why you need to delete this record.'\n)\ndef delete_record(record_id: int) -&gt; str:\n    \"\"\"Delete a record from the database.\n\n    This is a destructive operation and requires reasoning.\n\n    Args:\n        record_id: ID of the record to delete\n    \"\"\"\n    # Agent must provide reasoning before calling\n    return f\"Deleted record {record_id}\"\n\n# Agent interaction:\n# Thought: I need to delete record 123\n# Reasoning: The user requested to remove their old account\n# Action: delete_record(record_id=123)\n# Observation: Deleted record 123\n</code></pre>"},{"location":"guides/custom-tools/#jit-tools","title":"JIT Tools","text":"<p>Generate code at runtime:</p> <pre><code>from tinygent.tools import jit_tool\n\n@jit_tool(\n    jit_instruction='Generate code to process data according to user requirements.'\n)\ndef dynamic_processor(data: str):\n    \"\"\"Dynamically process data based on agent-generated code.\"\"\"\n    # Agent generates and executes code\n    yield from process_data(data)\n\n# Agent can adapt behavior at runtime\n</code></pre>"},{"location":"guides/custom-tools/#real-world-examples","title":"Real-World Examples","text":""},{"location":"guides/custom-tools/#1-web-scraper","title":"1. Web Scraper","text":"<pre><code>from bs4 import BeautifulSoup\nimport httpx\n\nclass ScraperInput(TinyModel):\n    url: str = Field(..., description='URL to scrape')\n    selector: str = Field('p', description='CSS selector for content')\n\n@register_tool\nasync def scrape_webpage(data: ScraperInput) -&gt; str:\n    \"\"\"Scrape content from a webpage.\n\n    Args:\n        url: The webpage URL\n        selector: CSS selector for elements to extract\n\n    Returns:\n        Extracted text content\n    \"\"\"\n    try:\n        async with httpx.AsyncClient() as client:\n            response = await client.get(data.url, timeout=10.0)\n            response.raise_for_status()\n\n        soup = BeautifulSoup(response.text, 'html.parser')\n        elements = soup.select(data.selector)\n        content = '\\n'.join(el.get_text(strip=True) for el in elements)\n\n        return content or \"No content found with that selector\"\n\n    except Exception as e:\n        raise Exception(f\"Scraping failed: {e}\")\n</code></pre>"},{"location":"guides/custom-tools/#2-file-operations","title":"2. File Operations","text":"<pre><code>from pathlib import Path\n\n@register_tool\ndef read_file(filepath: str) -&gt; str:\n    \"\"\"Read contents of a file.\n\n    Args:\n        filepath: Path to the file to read\n\n    Returns:\n        File contents as string\n    \"\"\"\n    try:\n        path = Path(filepath)\n        if not path.exists():\n            raise FileNotFoundError(f\"File not found: {filepath}\")\n\n        return path.read_text(encoding='utf-8')\n\n    except Exception as e:\n        raise Exception(f\"Error reading file: {e}\")\n\n@register_tool\ndef write_file(filepath: str, content: str) -&gt; str:\n    \"\"\"Write content to a file.\n\n    Args:\n        filepath: Path to the file to write\n        content: Content to write to the file\n\n    Returns:\n        Success message\n    \"\"\"\n    try:\n        path = Path(filepath)\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(content, encoding='utf-8')\n\n        return f\"Successfully wrote {len(content)} characters to {filepath}\"\n\n    except Exception as e:\n        raise Exception(f\"Error writing file: {e}\")\n</code></pre>"},{"location":"guides/custom-tools/#3-database-query","title":"3. Database Query","text":"<pre><code>import sqlite3\nfrom typing import List, Dict\n\nclass QueryInput(TinyModel):\n    sql: str = Field(..., description='SQL query to execute')\n    database: str = Field('app.db', description='Database file path')\n\n@register_tool\ndef query_database(data: QueryInput) -&gt; List[Dict]:\n    \"\"\"Execute a SQL query and return results.\n\n    Args:\n        sql: The SQL query to execute (SELECT only)\n        database: Path to SQLite database file\n\n    Returns:\n        List of result rows as dictionaries\n    \"\"\"\n    # Security: Only allow SELECT queries\n    if not data.sql.strip().upper().startswith('SELECT'):\n        raise ValueError(\"Only SELECT queries are allowed\")\n\n    try:\n        conn = sqlite3.connect(data.database)\n        conn.row_factory = sqlite3.Row\n        cursor = conn.cursor()\n\n        cursor.execute(data.sql)\n        results = [dict(row) for row in cursor.fetchall()]\n\n        conn.close()\n\n        return results\n\n    except Exception as e:\n        raise Exception(f\"Database error: {e}\")\n</code></pre>"},{"location":"guides/custom-tools/#4-api-client","title":"4. API Client","text":"<pre><code>class APICallInput(TinyModel):\n    endpoint: str = Field(..., description='API endpoint path')\n    method: str = Field('GET', description='HTTP method')\n    params: dict = Field(default_factory=dict, description='Query parameters')\n\n@register_tool\nasync def call_rest_api(data: APICallInput) -&gt; dict:\n    \"\"\"Call a REST API endpoint.\n\n    Args:\n        endpoint: API endpoint path\n        method: HTTP method (GET, POST, etc.)\n        params: Query parameters or JSON body\n\n    Returns:\n        API response as dictionary\n    \"\"\"\n    base_url = \"https://api.example.com\"\n    url = f\"{base_url}/{data.endpoint.lstrip('/')}\"\n\n    try:\n        async with httpx.AsyncClient() as client:\n            if data.method.upper() == 'GET':\n                response = await client.get(url, params=data.params)\n            elif data.method.upper() == 'POST':\n                response = await client.post(url, json=data.params)\n            else:\n                raise ValueError(f\"Unsupported method: {data.method}\")\n\n            response.raise_for_status()\n            return response.json()\n\n    except Exception as e:\n        raise Exception(f\"API call failed: {e}\")\n</code></pre>"},{"location":"guides/custom-tools/#best-practices","title":"Best Practices","text":""},{"location":"guides/custom-tools/#1-clear-descriptions","title":"1. Clear Descriptions","text":"<pre><code># Bad\n@tool\ndef process(data: str) -&gt; str:\n    \"\"\"Process data.\"\"\"  # Too vague\n    return data.upper()\n\n# Good\n@tool\ndef convert_to_uppercase(text: str) -&gt; str:\n    \"\"\"Convert text to uppercase letters.\n\n    Args:\n        text: The text to convert\n\n    Returns:\n        The text in uppercase\n\n    Example:\n        convert_to_uppercase(\"hello\") \u2192 \"HELLO\"\n    \"\"\"\n    return text.upper()\n</code></pre>"},{"location":"guides/custom-tools/#2-validate-inputs","title":"2. Validate Inputs","text":"<pre><code>from pydantic import Field, field_validator\n\nclass SearchInput(TinyModel):\n    query: str = Field(..., min_length=1, max_length=200)\n    limit: int = Field(10, ge=1, le=100)\n\n    @field_validator('query')\n    def clean_query(cls, v):\n        # Remove extra whitespace\n        return ' '.join(v.split())\n\n@register_tool\ndef search(data: SearchInput) -&gt; str:\n    \"\"\"Search with validated inputs.\"\"\"\n    return f\"Searching for: {data.query}\"\n</code></pre>"},{"location":"guides/custom-tools/#3-return-structured-data","title":"3. Return Structured Data","text":"<pre><code>from typing import List, Dict\n\n@tool\ndef search_products(category: str) -&gt; List[Dict[str, any]]:\n    \"\"\"Search for products.\n\n    Returns structured data for easy parsing.\n    \"\"\"\n    return [\n        {'id': 1, 'name': 'Product A', 'price': 99.99},\n        {'id': 2, 'name': 'Product B', 'price': 149.99},\n    ]\n</code></pre>"},{"location":"guides/custom-tools/#4-handle-edge-cases","title":"4. Handle Edge Cases","text":"<pre><code>@tool\ndef divide_numbers(a: float, b: float) -&gt; float:\n    \"\"\"Divide two numbers with edge case handling.\"\"\"\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n\n    if abs(b) &lt; 1e-10:\n        raise ValueError(\"Divisor too close to zero\")\n\n    result = a / b\n\n    if not math.isfinite(result):\n        raise ValueError(\"Result is infinite or NaN\")\n\n    return result\n</code></pre>"},{"location":"guides/custom-tools/#5-use-caching-wisely","title":"5. Use Caching Wisely","text":"<pre><code># Cache deterministic, expensive operations\n@register_tool(use_cache=True)\ndef calculate_fibonacci(n: int) -&gt; int:\n    \"\"\"Calculate nth Fibonacci number (cached).\"\"\"\n    # Expensive, but deterministic\n    if n &lt;= 1:\n        return n\n    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\n\n# Don't cache non-deterministic operations\n@register_tool(use_cache=False)  # Explicitly disable\ndef get_current_time() -&gt; str:\n    \"\"\"Get current timestamp (not cached).\"\"\"\n    # Non-deterministic - changes every call\n    return datetime.now().isoformat()\n</code></pre>"},{"location":"guides/custom-tools/#testing-tools","title":"Testing Tools","text":""},{"location":"guides/custom-tools/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_add_tool():\n    \"\"\"Test the add tool.\"\"\"\n    result = add(a=2, b=3)\n    assert result == 5\n\ndef test_add_with_dict():\n    \"\"\"Test calling with dict input.\"\"\"\n    result = add({'a': 5, 'b': 7})\n    assert result == 12\n\ndef test_validation_error():\n    \"\"\"Test validation errors.\"\"\"\n    with pytest.raises(ValueError):\n        divide(a=10, b=0)\n</code></pre>"},{"location":"guides/custom-tools/#integration-tests","title":"Integration Tests","text":"<pre><code>async def test_tool_with_agent():\n    \"\"\"Test tool integration with agent.\"\"\"\n    agent = build_agent(\n        'react',\n        llm='openai:gpt-4o-mini',\n        tools=[add, multiply]\n    )\n\n    result = agent.run('What is 5 + 3 multiplied by 2?')\n    assert '16' in result\n</code></pre>"},{"location":"guides/custom-tools/#next-steps","title":"Next Steps","text":"<ul> <li>Building Agents: Use your tools with agents</li> <li>Tool Concepts: Deep dive into tools</li> <li>Examples: More tool examples</li> </ul>"},{"location":"guides/custom-tools/#further-reading","title":"Further Reading","text":"<ul> <li>Tool Implementation: See <code>tinygent/tools/</code> for tool decorators</li> <li>Tool Catalog: See <code>tinygent/core/runtime/tool_catalog.py</code> for registry</li> <li>Real Tools: Check <code>packages/tiny_brave/</code> for a production tool example</li> </ul>"}]}